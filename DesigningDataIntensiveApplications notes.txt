Designing Data-Intensive Applications - THE BIG IDEAS BEHIND RELIABLE, SCALABLE, AND MAINTAINABLE SYSTEMS



We call an application data-intensive if data
is its primary challenge—the quantity of data, the complexity of data, or the speed at
which it is changing—as opposed to compute-intensive, where CPU cycles are the bottleneck.

The goal of this book is to help you navigate the diverse and fast-changing landscape
of technologies for processing and storing data. This book is not a tutorial for one
particular tool, nor is it a textbook full of dry theory. Instead, we will look at examples
of successful data systems: technologies that form the foundation of many popular
applications and that have to meet scalability, performance, and reliability require‐
ments in production every day



========================================================================================================================
========================================================================================================================
========================================================================================================================
========================================================================================================================
PART I Foundations of Data Systems




========================================================================================================================
Chapter 1. Reliable, Scalable, and Maintainable Applications

    Many applications today are data-intensive, as opposed to compute-intensive. Raw
    CPU power is rarely a limiting factor for these applications—bigger problems are
    usually the amount of data, the complexity of data, and the speed at which it is
    changing.Many applications today are data-intensive, as opposed to compute-intensive. Raw
    CPU power is rarely a limiting factor for these applications—bigger problems are
    usually the amount of data, the complexity of data, and the speed at which it is
    changing.

!!!!A data-intensive application is typically built from standard building blocks that pro‐
    vide commonly needed functionality. For example, many applications need to:
    • Store data so that they, or another application, can find it again later (databases)
    • Remember the result of an expensive operation, to speed up reads (caches)
    • Allow users to search data by keyword or filter it in various ways (search indexes)
    • Send a message to another process, to be handled asynchronously (stream processing)
    • Periodically crunch a large amount of accumulated data (batch processing)

    In this chapter, we will start by exploring the fundamentals of what we are trying to
    achieve: reliable, scalable, and maintainable data systems

    Thinking About Data Systems
        We typically think of databases, queues, caches, etc. as being very different categories
        of tools. Although a database and a message queue have some superficial similarity—
        both store data for some time—they have very different access patterns, which means
        different performance characteristics, and thus very different implementations.

!!!!    Many new tools for data storage and processing have emerged in recent years. They
        are optimized for a variety of different use cases, and they no longer neatly fit into
        traditional categories [1]. For example, there are datastores that are also used as mes‐
        sage queues (Redis), and there are message queues with database-like durability guar‐
        antees (Apache Kafka). The boundaries between the categories are becoming blurred.

        When you combine several tools in order to provide a service, the service’s interface
        or application programming interface (API) usually hides those implementation
        details from clients. Now you have essentially created a new, special-purpose data
        system from smaller, general-purpose components..... You are now not only an
        application developer, but also a data system designer.

        In this book, we focus on three concerns that are important in most software systems:
            Reliability
                The system should continue to work correctly (performing the correct function at
                the desired level of performance) even in the face of adversity (hardware or soft‐
                ware faults, and even human error)
            Scalability
                As the system grows (in data volume, traffic volume, or complexity), there should
                be reasonable ways of dealing with that growth.
            Maintainability
                Over time, many different people will work on the system (engineering and oper‐
                ations, both maintaining current behavior and adapting the system to new use
                cases), and they should all be able to work on it productively.

    Reliability
!!!!!   Everybody has an intuitive idea of what it means for something to be reliable or unre‐
        liable. For software, typical expectations include:
        • The application performs the function that the user expected.
        • It can tolerate the user making mistakes or using the software in unexpected ways.
        • Its performance is good enough for the required use case, under the expected
        load and data volume.
        • The system prevents any unauthorized access and abuse.
        If all those things together mean “working correctly,” then we can understand relia‐
        bility as meaning, roughly, “continuing to work correctly, even when things go wrong.”

        The things that can go wrong are called faults, and systems that anticipate faults and
        can cope with them are called fault-tolerant or resilient

!!!!    Note that a fault is not the same as a failure [2]. A fault is usually defined as one com‐
        ponent of the system deviating from its spec, whereas a failure is when the system as a
        whole stops providing the required service to the user
        It is impossible to reduce the
        probability of a fault to zero; therefore it is usually best to design fault-tolerance
        mechanisms that prevent faults from causing failures.

        Counterintuitively, in such fault-tolerant systems, it can make sense to increase the
        rate of faults by triggering them deliberately—for example, by randomly killing indi‐
        vidual processes without warning. Many critical bugs are actually due to poor error
        handling [3]; by deliberately inducing faults, you ensure that the fault-tolerance
        machinery is continually exercised and tested, which can increase your confidence
!!!     that faults will be handled correctly when they occur naturally. The Netflix Chaos
        Monkey [4] is an example of this approach.

        Hardware Faults
            Hard disks are reported as having a mean time to failure (MTTF) of about 10 to 50
            years [5, 6]. Thus, on a storage cluster with 10,000 disks, we should expect on average
            one disk to die per day.

            in some cloud platforms such as
            Amazon Web Services (AWS) it is fairly common for virtual machine instances to
            become unavailable without warning [7], as the platforms are designed to prioritize
            flexibility and elasticity over single-machine reliability.

        Software Errors
            The bugs that cause these kinds of software faults often lie dormant for a long time
            until they are triggered by an unusual set of circumstances. In those circumstances, it
            is revealed that the software is making some kind of assumption about its environ‐
            ment—and while that assumption is usually true, it eventually stops being true for
            some reason

        Human Errors
            For example, one study of large internet services found that
            configuration errors by operators were the leading cause of outages, whereas hard‐
            ware faults (servers or network) played a role in only 10–25% of outages

            How do we make our systems reliable, in spite of unreliable humans? The best sys‐
            tems combine several approaches:
            • Design systems in a way that minimizes opportunities for error. For example,
            well-designed abstractions, APIs, and admin interfaces make it easy to do “the
            right thing” and discourage “the wrong thing.”
            • Decouple the places where people make the most mistakes from the places where
            they can cause failures. In particular, provide fully featured non-production
            sandbox environments where people can explore and experiment safely, using
            real data, without affecting real users.
            • Test thoroughly at all levels, from unit tests to whole-system integration tests and
            manual tests [3].
            • Allow quick and easy recovery from human errors, to minimize the impact in the
            case of a failure. For example, make it fast to roll back configuration changes, roll
            out new code gradually (so that any unexpected bugs affect only a small subset of
            users), and provide tools to recompute data (in case it turns out that the old com‐
            putation was incorrect).
            • Set up detailed and clear monitoring, such as performance metrics and error
            rates. In other engineering disciplines this is referred to as telemetry. (Once a
            rocket has left the ground, telemetry is essential for tracking what is happening,
            and for understanding failures [14].) Monitoring can show us early warning sig‐
            nals and allow us to check whether any assumptions or constraints are being vio‐
            lated.

    Scalability
        Even if a system is working reliably today, that doesn’t mean it will necessarily work
        reliably in the future. One common reason for degradation is increased load: perhaps
        the system has grown from 10,000 concurrent users to 100,000 concurrent users, or
        from 1 million to 10 million. Perhaps it is processing much larger volumes of data
        than it did before.
        Scalability is the term we use to describe a system’s ability to cope with increased
        load.

!!!!    considering questions like “If the system grows in a particular way,
        what are our options for coping with the growth?” and “How can we add computing
        resources to handle the additional load?”

        Describing Load
!!!!        Load can be described
            with a few numbers which we call load parameters. The best choice of parameters
            depends on the architecture of your system: it may be requests per second to a web
            server, the ratio of reads to writes in a database, the number of simultaneously active
            users in a chat room, the hit rate on a cache, or something else. Perhaps the average
            case is what matters for you, or perhaps your bottleneck is dominated by a small
            number of extreme cases.

            To make this idea more concrete, let’s consider Twitter as an example, using data
            published in November 2012 [16]. Two of Twitter’s main operations are:
                Post tweet
                    A user can publish a new message to their followers (4.6k requests/sec on aver‐
                    age, over 12k requests/sec at peak).
                Home timeline
                    A user can view tweets posted by the people they follow (300k requests/sec).
            Simply handling 12,000 writes per second (the peak rate for posting tweets) would be
!!          fairly easy. However, Twitter’s scaling challenge is not primarily due to tweet volume,
            but due to fan-out—each user follows many people, and each user is followed by
            many people.

                fan-out = In transaction processing systems, we use it to describe the number of requests to other services that we need
                          to make in order to serve one incoming request.

            Dan: see picture "1-3 twitters hybrid approach of handling load.png" and more explanations in the book
                about how a hybrid approach helped them with load

            Most users’ tweets continue to be
            fanned out to home timelines at the time when they are posted, but a small number
            of users with a very large number of followers (i.e., celebrities) are excepted from this
            fan-out. Tweets from any celebrities that a user may follow are fetched separately and
            merged with that user’s home timeline when it is read, like in approach 1. This hybrid
            approach is able to deliver consistently good performance

        Describing Performance
!!!!!       Once you have described the load on your system, you can investigate what happens
            when the load increases. You can look at it in two ways:
            • When you increase a load parameter and keep the system resources (CPU, mem‐
            ory, network bandwidth, etc.) unchanged, how is the performance of your system affected?
            • When you increase a load parameter, how much do you need to increase the
            resources if you want to keep performance unchanged?

!!          In a batch processing system such as Hadoop, we usually care about throughput—the
            number of records we can process per second, or the total time it takes to run a job
            on a dataset of a certain size. In online systems, what’s usually more important is the
            service’s response time—that is, the time between a client sending a request and
            receiving a response.

!!!         Latency and response time are often used synonymously, but they
            are not the same. The response time is what the client sees: besides
            the actual time to process the request (the service time), it includes
            network delays and queueing delays. Latency is the duration that a
            request is waiting to be handled—during which it is latent, await‐
            ing service

!!!         In practice, in a system handling a variety of
            requests, the response time can vary a lot. We therefore need to think of response
            time not as a single number, but as a distribution of values that you can measure

!!          In order to figure out how bad your outliers are, you can look at higher percentiles:
            the 95th, 99th, and 99.9th percentiles are common (abbreviated p95, p99, and p999).
            They are the response time thresholds at which 95%, 99%, or 99.9% of requests are
            faster than that particular threshold. For example, if the 95th percentile response time
            is 1.5 seconds, that means 95 out of 100 requests take less than 1.5 seconds, and 5 out
            of 100 requests take 1.5 seconds or more.

            High percentiles of response times, also known as tail latencies, are important
            because they directly affect users’ experience of the service. For example, Amazon
            describes response time requirements for internal services in terms of the 99.9th per‐
            centile, even though it only affects 1 in 1,000 requests. This is because the customers
            with the slowest requests are often those who have the most data on their accounts
            because they have made many purchases—that is, they’re the most valuable custom‐
            ers. It’s important to keep those customers happy by ensuring the website is fast
            for them: Amazon has also observed that a 100 ms increase in response time reduces
            sales by 1%, and others report that a 1-second slowdown reduces a customer sat‐
            isfaction metric by 16%
            On the other hand, optimizing the 99.99th percentile (the slowest 1 in 10,000
            requests) was deemed too expensive and to not yield enough benefit for Amazon’s
            purposes. Reducing response times at very high percentiles is difficult because they
            are easily affected by random events outside of your control, and the benefits are
            diminishing.

            When generating load artificially in order to test the scalability of a system, the load
            generating client needs to keep sending requests independently of the response time.
            If the client waits for the previous request to complete before sending the next one,
            that behavior has the effect of artificially keeping the queues shorter in the test than
            they would be in reality, which skews the measurements

            Dan: see "1-5 one slow request destroys user experience.png"

        Approaches for Coping with Load
            An architecture that is appropriate for one level of load is unlikely to cope with 10
            times that load. If you are working on a fast-growing service, it is therefore likely that
            you will need to rethink your architecture on every order of magnitude load increase
            —or perhaps even more often than that.

            People often talk of a dichotomy between scaling up (vertical scaling, moving to a
            more powerful machine) and scaling out (horizontal scaling, distributing the load
            across multiple smaller machines). Distributing load across multiple machines is also
            known as a shared-nothing architecture. A system that can run on a single machine is
            often simpler, but high-end machines can become very expensive, so very intensive
            workloads often can’t avoid scaling out.

            Some systems are elastic, meaning that they can automatically add computing resour‐
            ces when they detect a load increase, whereas other systems are scaled manually (a
            human analyzes the capacity and decides to add more machines to the system). An
            elastic system can be useful if load is highly unpredictable, but manually scaled sys‐
            tems are simpler and may have fewer operational surprise

!!!         While distributing stateless services across multiple machines is fairly straightfor‐
            ward, taking stateful data systems from a single node to a distributed setup can intro‐
            duce a lot of additional complexity. For this reason, common wisdom until recently
            was to keep your database on a single node (scale up) until scaling cost or high availability
            requirements forced you to make it distributed.

!!!!        The architecture of systems that operate at large scale is usually highly specific to the
            application—there is no such thing as a generic, one-size-fits-all scalable architecture
            (informally known as magic scaling sauce). The problem may be the volume of reads,
            the volume of writes, the volume of data to store, the complexity of the data, the
            response time requirements, the access patterns, or (usually) some mixture of all of
            these plus many more issues.
            For example, a system that is designed to handle 100,000 requests per second, each
            1 kB in size, looks very different from a system that is designed for 3 requests per
            minute, each 2 GB in size—even though the two systems have the same data through‐
            put.

!!!         An architecture that scales well for a particular application is built around assump‐
            tions of which operations will be common and which will be rare—the load parame‐
            ters. If those assumptions turn out to be wrong, the engineering effort for scaling is at
            best wasted, and at worst counterproductive. In an early-stage startup or an unpro‐
            ven product it’s usually more important to be able to iterate quickly on product fea‐
            tures than it is to scale to some hypothetical future load.

    Maintainability
        It is well known that the majority of the cost of software is not in its initial develop‐
        ment, but in its ongoing maintenance—fixing bugs, keeping its systems operational,
        investigating failures, adapting it to new platforms

!!      Yet, unfortunately, many people working on software systems dislike maintenance of
        so-called legacy systems—perhaps it involves fixing other people’s mistakes, or work‐
        ing with platforms that are now outdated, or systems that were forced to do things
        they were never intended for. Every legacy system is unpleasant in its own way, and
        so it is difficult to give general recommendations for dealing with them

        To this end, we will pay particular attention to three design principles for software
        systems:
            Operability
                Make it easy for operations teams to keep the system running smoothly.
            Simplicity
                Make it easy for new engineers to understand the system, by removing as much
                complexity as possible from the system. (Note this is not the same as simplicity
                of the user interface.)
            Evolvability
                Make it easy for engineers to make changes to the system in the future, adapting
                it for unanticipated use cases as requirements change. Also known as extensibil‐
                ity, modifiability, or plasticity


========================================================================================================================
Chapter 2. Data Models and Query Languages

    Data models are perhaps the most important part of developing software, because
    they have such a profound effect: not only on how the software is written, but also on
    how we think about the problem that we are solving

    Most applications are built by layering one data model on top of another. For each
    layer, the key question is: how is it represented in terms of the next-lower layer? For
    example:
    1. As an application developer, you look at the real world (in which there are peo‐
    ple, organizations, goods, actions, money flows, sensors, etc.) and model it in
    terms of objects or data structures, and APIs that manipulate those data struc‐
    tures. Those structures are often specific to your application.
    2. When you want to store those data structures, you express them in terms of a
    general-purpose data model, such as JSON or XML documents, tables in a rela‐
    tional database, or a graph model.
    3. The engineers who built your database software decided on a way of representing
    that JSON/XML/relational/graph data in terms of bytes in memory, on disk, or
    on a network. The representation may allow the data to be queried, searched,
    manipulated, and processed in various ways.
    4. On yet lower levels, hardware engineers have figured out how to represent bytes
    in terms of electrical currents, pulses of light, magnetic fields, and more.

!!!! It can take a lot of effort to master just one data model (think how many books there
    are on relational data modeling). Building software is hard enough, even when work‐
    ing with just one data model and without worrying about its inner workings. But
    since the data model has such a profound effect on what the software above it can
    and can’t do, it’s important to choose one that is appropriate to the application

!!  In particular, we will compare the
    relational model, the document model, and a few graph-based data models. We will
    also look at various query languages and compare their use cases.

    Relational Model Versus Document Model
!!      The best-known data model today is probably that of SQL, based on the relational
        model proposed by Edgar Codd in 1970 [1]: data is organized into relations (called
        tables in SQL), where each relation is an unordered collection of tuples (rows in SQL).

        by the mid-1980s, relational database management systems (RDBMSes) and SQL had become the tools
        of choice for most people who needed to store and query data with some kind of reg‐
        ular structure. The dominance of relational databases has lasted around 25‒30 years
        —an eternity in computing history.

        Object databases came and went again in the late 1980s and early 1990s. XML databases
        appeared in the early 2000s, but have only seen niche adoption. Each competitor to
        the relational model generated a lot of hype in its time, but it never lasted

        Much of what you see on the web today is still powered by
        relational databases, be it online publishing, discussion, social networking, ecom‐
        merce, games, software-as-a-service productivity applications, or much more.

        The Birth of NoSQL
            Now, in the 2010s, NoSQL is the latest attempt to overthrow the relational model’s
            dominance. The name “NoSQL” is unfortunate, since it doesn’t actually refer to any
            particular technology—it was originally intended simply as a catchy Twitter hashtag
            for a meetup on open source, distributed, nonrelational databases in 2009 [3]. Never‐
            theless, the term struck a nerve and quickly spread through the web startup commu‐
            nity and beyond. A number of interesting database systems are now associated with
            the #NoSQL hashtag, and it has been retroactively reinterpreted as Not Only SQL

            There are several driving forces behind the adoption of NoSQL databases, including:
                • A need for greater scalability than relational databases can easily achieve, includ‐
                ing very large datasets or very high write throughput
                • A widespread preference for free and open source software over commercial
                database products
                • Specialized query operations that are not well supported by the relational model
                • Frustration with the restrictiveness of relational schemas, and a desire for a more
                dynamic and expressive data model

        The Object-Relational Mismatch
            Most application development today is done in object-oriented programming lan‐
            guages, which leads to a common criticism of the SQL data model: if data is stored in
            relational tables, an awkward translation layer is required between the objects in the
            application code and the database model of tables, rows, and columns.

!!!         Object-relational mapping (ORM) frameworks like ActiveRecord and Hibernate
            reduce the amount of boilerplate code required for this translation layer, but they
            can’t completely hide the differences between the two models.

            if one wants to represent a resume,
            In the traditional SQL model (prior to SQL:1999), the most common normalized
            representation is to put positions, education, and contact information in separate
            tables, with a foreign key reference to the users table, as in Figure 2-1.

!!!!        For a data structure like a résumé, which is mostly a self-contained document, a JSON
            representation can be quite appropriate: see Example 2-1. JSON has the appeal of
            being much simpler than XML. Document-oriented databases like MongoDB [9],
            RethinkDB [10], CouchDB [11], and Espresso [12] support this data model.

            The JSON representation has better locality than the multi-table schema in
            Figure 2-1. If you want to fetch a profile in the relational example, you need to either
            perform multiple queries (query each table by user_id) or perform a messy multiway join between the users
            table and its subordinate tables. In the JSON representa‐
            tion, all the relevant information is in one place, and one query is sufficient

        Many-to-One and Many-to-Many Relationships
            Whether you store an ID or a text string is a question of duplication. When you use
            an ID, the information that is meaningful to humans (such as the word "Philanthropy")
            is stored in only one place, and everything that refers to it uses an ID (which only has
            meaning within the database). When you store the text directly, you are duplicating
            the human-meaningful information in every record that uses it.

            The advantage of using an ID is that because it has no meaning to humans, it never
            needs to change: the ID can remain the same, even if the information it identifies
            changes. Anything that is meaningful to humans may need to change sometime in
            the future—and if that information is duplicated, all the redundant copies need to be
            updated. That incurs write overheads, and risks inconsistencies (where some copies
            of the information are updated but others aren’t). Removing such duplication is the
            key idea behind normalization in databases.

!!!!!!!     As a rule of thumb, if you’re duplicating values that could be stored in just one place,
            the schema is not normalized.

            Unfortunately, normalizing this data requires many-to-one relationships (many peo‐
            ple live in one particular region, many people work in one particular industry), which
            don’t fit nicely into the document model. In relational databases, it’s normal to refer
            to rows in other tables by ID, because joins are easy. In document databases, joins are
            not needed for one-to-many tree structures, and support for joins is often weak

!!!         If the database itself does not support joins, you have to emulate a join in application
            code by making multiple queries to the database. (In this case, the lists of regions and
            industries are probably small and slow-changing enough that the application can
            simply keep them in memory. But nevertheless, the work of making the join is shifted
            from the database to the application code.)

            Dan: see "2-4 extending resumes with many to many relationships.png" ..where because of needed
            changes, the CV document storage needs to support joins as well, for schools, organizations and
            other users

        Are Document Databases Repeating History?
!!!         In a relational database, the query optimizer automatically decides which parts of the
            query to execute in which order, and which indexes to use. Those choices are effec‐
            tively the “access path,” but the big difference is that they are made automatically by
            the query optimizer, not by the application developer, so we rarely need to think
            about them.
            If you want to query your data in new ways, you can just declare a new index, and
            queries will automatically use whichever indexes are most appropriate. You don’t
            need to change your queries to take advantage of a new index. (See also “Query Lan‐
            guages for Data” on page 42.) The relational model thus made it much easier to add
            new features to applications.

            However, when it comes to representing many-to-one and many-to-many relation‐
            ships, relational and document databases are not fundamentally different: in both
            cases, the related item is referenced by a unique identifier, which is called a foreign
            key in the relational model and a document reference in the document model [9].
            That identifier is resolved at read time by using a join or follow-up queries.

!!!!    Relational Versus Document Databases Today
!!!!        There are many differences to consider when comparing relational databases to
            document databases, including their fault-tolerance properties (see Chapter 5) and
            handling of concurrency (see Chapter 7). In this chapter, we will concentrate only on
            the differences in the data model.
            The main arguments in favor of the document data model are schema flexibility, bet‐
            ter performance due to locality, and that for some applications it is closer to the data
            structures used by the application. The relational model counters by providing better
            support for joins, and many-to-one and many-to-many relationships.

!!!!        Which data model leads to simpler application code?
                If the data in your application has a document-like structure (i.e., a tree of one-to many
                relationships, where typically the entire tree is loaded at once), then it’s proba‐
                bly a good idea to use a document model. The relational technique of shredding—
                splitting a document-like structure into multiple tables (like positions, education,
                and contact_info in Figure 2-1)—can lead to cumbersome schemas and unnecessa‐
                rily complicated application code.

                The document model has limitations: for example, you cannot refer directly to a nes‐
                ted item within a document, but instead you need to say something like “the second
                item in the list of positions for user 251” (much like an access path in the hierarchical
                model). However, as long as documents are not too deeply nested, that is not usually
                a problem.

                The poor support for joins in document databases may or may not be a problem,
                depending on the application. For example, many-to-many relationships may never
                be needed in an analytics application that uses a document database to record which
                events occurred at which time

!!!!            However, if your application does use many-to-many relationships, the document
                model becomes less appealing. It’s possible to reduce the need for joins by denormal‐
                izing, but then the application code needs to do additional work to keep the denor‐
                malized data consistent. Joins can be emulated in application code by making
                multiple requests to the database, but that also moves complexity into the application
                and is usually slower than a join performed by specialized code inside the database.
                In such cases, using a document model can lead to significantly more complex appli‐
                cation code and worse performance

                For highly interconnected data, the document model is awkward, the relational model is accept‐
                able, and graph models (see “Graph-Like Data Models” on page 49) are the most
                natural.

            Schema flexibility in the document model
!!!!            Most document databases, and the JSON support in relational databases, do not
                enforce any schema on the data in documents. XML support in relational databases
                usually comes with optional schema validation. No schema means that arbitrary keys
                and values can be added to a document, and when reading, clients have no guaran‐
                tees as to what fields the documents may contain.
                Document databases are sometimes called schemaless, but that’s misleading, as the
                code that reads the data usually assumes some kind of structure—i.e., there is an
                implicit schema, but it is not enforced by the database. A more accurate term is
                schema-on-read (the structure of the data is implicit, and only interpreted when the
                data is read), in contrast with schema-on-write (the traditional approach of relational

!!!             Schema-on-read is similar to dynamic (runtime) type checking in programming lan‐
                guages, whereas schema-on-write is similar to static (compile-time) type checking.
                Just as the advocates of static and dynamic type checking have big debates about their
                relative merits [22], enforcement of schemas in database is a contentious topic, and in
                general there’s no right or wrong answer.

                The difference between the approaches is particularly noticeable in situations where
                an application wants to change the format of its data. For example, say you are cur‐
                rently storing each user’s full name in one field, and you instead want to store the
                first name and last name separately. In a document database, you would just
                start writing new documents with the new fields and have code in the application that
                handles the case when old documents are read.

                Running the UPDATE statement on a large table is likely to be slow on any database,
                since every row needs to be rewritten. If that is not acceptable, the application can
                leave first_name set to its default of NULL and fill it in at read time, like it would with
                a document database.

            Data locality for queries
                A document is usually stored as a single continuous string, encoded as JSON, XML,
!!!!            or a binary variant thereof (such as MongoDB’s BSON). If your application often
                needs to access the entire document (for example, to render it on a web page), there is
                a performance advantage to this storage locality. If data is split across multiple tables,
                like in Figure 2-1, multiple index lookups are required to retrieve it all, which may
                require more disk seeks and take more time.
                The locality advantage only applies if you need large parts of the document at the
                same time. The database typically needs to load the entire document, even if you
                access only a small portion of it, which can be wasteful on large documents. On
                updates to a document, the entire document usually needs to be rewritten—only
                modifications that don’t change the encoded size of a document can easily be per‐
                formed in place [19]. For these reasons, it is generally recommended that you keep
                documents fairly small and avoid writes that increase the size of a document [9].
                These performance limitations significantly reduce the set of situations in which
                document databases are useful.

        Query Languages for Data
!!!         An imperative language tells the computer to perform certain operations in a certain
            order. You can imagine stepping through the code line by line, evaluating conditions,
            updating variables, and deciding whether to go around the loop one more time.

!!!         In a declarative query language, like SQL or relational algebra, you just specify the
            pattern of the data you want—what conditions the results must meet, and how you
            want the data to be transformed (e.g., sorted, grouped, and aggregated)—but not how
            to achieve that goal. It is up to the database system’s query optimizer to decide which
            indexes and which join methods to use, and in which order to execute various parts
            of the query.

            A declarative query language is attractive because it is typically more concise and eas‐
            ier to work with than an imperative API. But more importantly, it also hides imple‐
            mentation details of the database engine, which makes it possible for the database
            system to introduce performance improvements without requiring any changes to
            queries.

            Finally, declarative languages often lend themselves to parallel execution. Today,
            CPUs are getting faster by adding more cores, not by running at significantly higher
            clock speeds than before [31]. Imperative code is very hard to parallelize across mul‐
            tiple cores and multiple machines, because it specifies instructions that must be per‐
            formed in a particular order. Declarative languages have a better chance of getting
            faster in parallel execution because they specify only the pattern of the results, not the
            algorithm that is used to determine the results

            Declarative Queries on the Web
                CSS
                In a web browser, using declarative CSS styling is much better than manipulating
                styles imperatively in JavaScript. Similarly, in databases, declarative query languages
                like SQL turned out to be much better than imperative query APIs.

            MapReduce Querying
                MapReduce is a programming model for processing large amounts of data in bulk
                across many machines, popularized by Google
                A limited form of MapReduce is
                supported by some NoSQL datastores, including MongoDB and CouchDB, as a
                mechanism for performing read-only queries across many documents.

                MapReduce is neither a declarative query language nor a fully imperative query API,
                but somewhere in between: the logic of the query is expressed with snippets of code,
                which are called repeatedly by the processing framework. It is based on the map (also
                known as collect) and reduce (also known as fold or inject) functions that exist
                in many functional programming languages.

!!!!            The map and reduce functions are somewhat restricted in what they are allowed to
                do. They must be pure functions, which means they only use the data that is passed to
                them as input, they cannot perform additional database queries, and they must not
                have any side effects. These restrictions allow the database to run the functions any‐
                where, in any order, and rerun them on failure. However, they are nevertheless pow‐
                erful: they can parse strings, call library functions, perform calculations, and more

        Graph-Like Data Models
            A graph consists of two kinds of objects: vertices (also known as nodes or entities) and
            edges (also known as relationships or arcs). Many kinds of data can be modeled as a
            graph. Typical examples include:
                Social graphs
                    Vertices are people, and edges indicate which people know each other.
                The web graph
                    Vertices are web pages, and edges indicate HTML links to other pages.
                Road or rail networks
                    Vertices are junctions, and edges represent the roads or railway lines between them.

!!!!!       In the examples just given, all the vertices in a graph represent the same kind of thing
            (people, web pages, or road junctions, respectively). However, graphs are not limited
            to such homogeneous data: an equally powerful use of graphs is to provide a consis‐
            tent way of storing completely different types of objects in a single datastore. For
            example, Facebook maintains a single graph with many different types of vertices and
            edges: vertices represent people, locations, events, checkins, and comments made by
            users; edges indicate which people are friends with each other, which checkin hap‐
            pened in which location, who commented on which post, who attended which event,
            and so on
                Dan: see "2-5 example of Graph data that contains different types of nodes.png"

            There are several different, but related, ways of structuring and querying data in
            graphs. In this section we will discuss the property graph model (implemented by
            Neo4j, Titan, and InfiniteGraph) and the triple-store model (implemented by
            Datomic, AllegroGraph, and others).

            Property Graphs
!!!!           In the property graph model, each vertex consists of:
                • A unique identifier
                • A set of outgoing edges
                • A set of incoming edges
                • A collection of properties (key-value pairs)

!!!!           Each edge consists of:
                • A unique identifier
                • The vertex at which the edge starts (the tail vertex)
                • The vertex at which the edge ends (the head vertex)
                • A label to describe the kind of relationship between the two vertices
                • A collection of properties (key-value pairs)

                Those features give graphs a great deal of flexibility for data modeling, as illustrated
                in Figure 2-5. The figure shows a few things that would be difficult to express in a
                traditional relational schema, such as different kinds of regional structures in differ‐
                ent countries (France has départements and régions, whereas the US has counties and
                states

!!!             Graphs are good for evolvability: as you
                add features to your application, a graph can easily be extended to accommodate
                changes in your application’s data structures.

            The Cypher Query Language
!!!             Cypher is a declarative query language for property graphs, created for the Neo4j
                graph database [37]. (It is named after a character in the movie The Matrix and is not
                related to ciphers in cryptography

            Triple-Stores and SPARQL
                The triple-store model is mostly equivalent to the property graph model, using differ‐
                ent words to describe the same ideas. It is nevertheless worth discussing, because
                there are various tools and languages for triple-stores that can be valuable additions
                to your toolbox for building applications.
                In a triple-store, all information is stored in the form of very simple three-part state‐
                ments: (subject, predicate, object). For example, in the triple (Jim, likes, bananas), Jim
                is the subject, likes is the predicate (verb), and bananas is the object.

                The subject of a triple is equivalent to a vertex in a graph. The object is one of two
                things:
                    1. A value in a primitive datatype, such as a string or a number. In that case, the
                    predicate and object of the triple are equivalent to the key and value of a property
                    on the subject vertex. For example, (lucy, age, 33) is like a vertex lucy with prop‐
                    erties {"age":33}.
                    2. Another vertex in the graph. In that case, the predicate is an edge in the graph,
                    the subject is the tail vertex, and the object is the head vertex. For example, in
                    (lucy, marriedTo, alain) the subject and object lucy and alain are both vertices,
                    and the predicate marriedTo is the label of the edge that connects them.

!!!!        All three models (document, relational, and graph) are widely used today, and each is
            good in its respective domain. One model can be emulated in terms of another model
            —for example, graph data can be represented in a relational database—but the result
            is often awkward. That’s why we have different systems for different purposes, not a
            single one-size-fits-all solution.


========================================================================================================================
Chapter 3. Storage and Retrieval

    On the most fundamental level, a database needs to do two things: when you give it
    some data, it should store the data, and when you ask it again later, it should give the
    data back to you.
    In Chapter 2 we discussed data models and query languages—i.e., the format in
    which you (the application developer) give the database your data, and the mecha‐
    nism by which you can ask for it again later. In this chapter we discuss the same from
    the database’s point of view: how we can store the data that we’re given, and how we
    can find it again when we’re asked for it.

!!!!
    Why should you, as an application developer, care how the database handles storage
    and retrieval internally? You’re probably not going to implement your own storage
    engine from scratch, but you do need to select a storage engine that is appropriate for
    your application, from the many that are available. In order to tune a storage engine
    to perform well on your kind of workload, you need to have a rough idea of what the
    storage engine is doing under the hood.

    Data Structures That Power Your Database
!!      many databases internally use a log, which is an append-only data file.
        Real databases have more issues to deal with (such as concurrency control, reclaim‐
        ing disk space so that the log doesn’t grow forever, and handling errors and partially
        written records), but the basic principle is the same. Logs are incredibly useful, and
        we will encounter them several times in the rest of this book.

!!      The word log is often used to refer to application logs, where an
        application outputs text that describes what’s happening. In this
        book, log is used in the more general sense: an append-only
        sequence of records. It doesn’t have to be human-readable; it might
        be binary and intended only for other programs to read.

!!!!    In order to efficiently find the value for a particular key in the database, we need a
        different data structure: an index. In this chapter we will look at a range of indexing
        structures and see how they compare; the general idea behind them is to keep some
        additional metadata on the side, which acts as a signpost and helps you to locate the
        data you want. If you want to search the same data in several different ways, you may
        need several different indexes on different parts of the data.

!!!!    An index is an additional structure that is derived from the primary data. Many data‐
        bases allow you to add and remove indexes, and this doesn’t affect the contents of the
        database; it only affects the performance of queries. Maintaining additional structures
        incurs overhead, especially on writes. For writes, it’s hard to beat the performance of
        simply appending to a file, because that’s the simplest possible write operation. Any
        kind of index usually slows down writes, because the index also needs to be updated
        every time data is written.

!!!!    This is an important trade-off in storage systems: well-chosen indexes speed up read
        queries, but every index slows down writes. For this reason, databases don’t usually
        index everything by default, but require you—the application developer or database
        administrator—to choose indexes manually, using your knowledge of the applica‐
        tion’s typical query patterns. You can then choose the indexes that give your applica‐
        tion the greatest benefit, without introducing more overhead than necessary

        Hash Indexes
            Key-value stores are quite similar to the dictionary type that you can find in most
            programming languages, and which is usually implemented as a hash map (hash
            table). Hash maps are described in many algorithms textbooks [1, 2], so we won’t go
            into detail of how they work here. Since we already have hash maps for our in memory data structures,
            why not use them to index our data on disk?
            Let’s say our data storage consists only of appending to a file, as in the preceding
            example. Then the simplest possible indexing strategy is this: keep an in-memory
            hash map where every key is mapped to a byte offset in the data file—the location at
            which the value can be found, as illustrated in Figure 3-1. Whenever you append a
            new key-value pair to the file, you also update the hash map to reflect the offset of the
            data you just wrote (this works both for inserting new keys and for updating existing
            keys). When you want to look up a value, use the hash map to find the offset in the
            data file, seek to that location, and read the value

            Dan: "3-1 index using an in memory hash map.png"

!!          A storage engine like Bitcask is well suited to situations where the value for each key
            is updated frequently. For example, the key might be the URL of a cat video, and the
            value might be the number of times it has been played (incremented every time
            someone hits the play button). In this kind of workload, there are a lot of writes, but
            there are not too many distinct keys—you have a large number of writes per key, but
            it’s feasible to keep all keys in memory.

            As described so far, we only ever append to a file—so how do we avoid eventually
            running out of disk space? A good solution is to break the log into segments of a cer‐
            tain size by closing a segment file when it reaches a certain size, and making subse‐
            quent writes to a new segment file. We can then perform compaction on these
            segments, as illustrated in Figure 3-2. Compaction means throwing away duplicate
            keys in the log, and keeping only the most recent update for each key

            Moreover, since compaction often makes segments much smaller (assuming that a
            key is overwritten several times on average within one segment), we can also merge
            several segments together at the same time as performing the compaction, as shown
            in Figure 3-3. Segments are never modified after they have been written, so the
            merged segment is written to a new file. The merging and compaction of frozen seg‐
            ments can be done in a background thread, and while it is going on, we can still con‐
            tinue to serve read and write requests as normal, using the old segment files. After the
            merging process is complete, we switch read requests to using the new merged seg‐
            ment instead of the old segments—and then the old segment files can simply be
            deleted.

            Each segment now has its own in-memory hash table, mapping keys to file offsets. In
            order to find the value for a key, we first check the most recent segment’s hash map;
            if the key is not present we check the second-most-recent segment, and so on. The
            merging process keeps the number of segments small, so lookups don’t need to check
            many hash maps.

            However, the hash table index also has limitations:
                • The hash table must fit in memory, so if you have a very large number of keys,
                you’re out of luck. In principle, you could maintain a hash map on disk, but
                unfortunately it is difficult to make an on-disk hash map perform well. It
                requires a lot of random access I/O, it is expensive to grow when it becomes full,
                and hash collisions require fiddly logic.
                • Range queries are not efficient. For example, you cannot easily scan over all keys
                between kitty00000 and kitty99999—you’d have to look up each key individu‐
                ally in the hash maps.

            In the next section we will look at an indexing structure that doesn’t have those limitations.

        SSTables and LSM-Trees
            Now we can make a simple change to the format of our segment files: we require that
            the sequence of key-value pairs is sorted by key. At first glance, that requirement
            seems to break our ability to use sequential writes, but we’ll get to that in a moment.

            We call this format Sorted String Table, or SSTable for short. We also require that
            each key only appears once within each merged segment file (the compaction process
            already ensures that). SSTables have several big advantages over log segments with
            hash indexes:
                1. Merging segments is simple and efficient, even if the files are bigger than the
                available memory. The approach is like the one used in the mergesort algorithm
                and is illustrated in Figure 3-4: you start reading the input files side by side, look
                at the first key in each file, copy the lowest key (according to the sort order) to
                the output file, and repeat. This produces a new merged segment file, also sorted
                by key

                2. In order to find a particular key in the file, you no longer need to keep an index
                of all the keys in memory. See Figure 3-5 for an example: say you’re looking for
                the key handiwork, but you don’t know the exact offset of that key in the segment
                file. However, you do know the offsets for the keys handbag and handsome, and
                because of the sorting you know that handiwork must appear between those two.
                This means you can jump to the offset for handbag and scan from there until you
                find handiwork (or not, if the key is not present in the file).
                You still need an in-memory index to tell you the offsets for some of the keys, but
                it can be sparse: one key for every few kilobytes of segment file is sufficient,
                because a few kilobytes can be scanned very quickly.

                3. Since read requests need to scan over several key-value pairs in the requested
                range anyway, it is possible to group those records into a block and compress it
                before writing it to disk (indicated by the shaded area in Figure 3-5). Each entry
                of the sparse in-memory index then points at the start of a compressed block.
                Besides saving disk space, compression also reduces the I/O bandwidth use.

            Making an LSM-tree out of SSTables
                The algorithm described here is essentially what is used in LevelDB [6] and RocksDB
                [7], key-value storage engine libraries that are designed to be embedded into other
                applications.

                Storage engines that are based on this principle of
                merging and compacting sorted files are often called LSM storage engines.

                Lucene, an indexing engine for full-text search used by Elasticsearch and Solr, uses a
                similar method for storing its term dictionary [12, 13]. A full-text index is much more
                complex than a key-value index but is based on a similar idea: given a word in a
                search query, find all the documents (web pages, product descriptions, etc.) that
                mention the word. This is implemented with a key-value structure where the key is a
                word (a term) and the value is the list of IDs of all the documents that contain the
                word (the postings list). In Lucene, this mapping from term to postings list is kept in
                SSTable-like sorted files, which are merged in the background as needed

        B-Trees
            The log-structured indexes we have discussed so far are gaining acceptance, but they
            are not the most common type of index. The most widely used indexing structure is
            quite different: the B-tree.

!!!!        B-trees have stood the test of time very well. They remain the standard index implementation
            in almost all relational databases, and many nonrelational databases use them too.

            Like SSTables, B-trees keep key-value pairs sorted by key, which allows efficient key value
            lookups and range queries. But that’s where the similarity ends: B-trees have a
            very different design philosophy

            Dan: see "3-6 looking for a key in a b tree index.png"

            One page is designated as the root of the B-tree; whenever you want to look up a key
            in the index, you start here. The page contains several keys and references to child
            pages. Each child is responsible for a continuous range of keys, and the keys between
            the references indicate where the boundaries between those ranges lie.
            In the example in Figure 3-6, we are looking for the key 251, so we know that we need
            to follow the page reference between the boundaries 200 and 300. That takes us to a
            similar-looking page that further breaks down the 200–300 range into subranges

            Eventually we get down to a page containing individual keys (a leaf page), which
            either contains the value for each key inline or contains references to the pages where
            the values can be found.
            The number of references to child pages in one page of the B-tree is called the
            branching factor. For example, in Figure 3-6 the branching factor is six. In practice,
            the branching factor depends on the amount of space required to store the page refer‐
            ences and the range boundaries, but typically it is several hundred.

!!!         ....This algorithm ensures that the tree remains balanced: a B-tree with n keys always
            has a depth of O(log n). Most databases can fit into a B-tree that is three or four levels
            deep, so you don’t need to follow many page references to find the page you are look‐
            ing for. (A four-level tree of 4 KB pages with a branching factor of 500 can store up to
            256 TB.)

        Comparing B-Trees and LSM-Trees
            B-trees are very ingrained in the architecture of databases and provide consistently
            good performance for many workloads, so it’s unlikely that they will go away anytime
            soon. In new datastores, log-structured indexes are becoming increasingly popular.
            There is no quick and easy rule for determining which type of storage engine is better
            for your use case, so it is worth testing empirically.

        Other Indexing Structures
!!!         So far we have only discussed key-value indexes, which are like a primary key index in
            the relational model. A primary key uniquely identifies one row in a relational table,
            or one document in a document database, or one vertex in a graph database. Other
            records in the database can refer to that row/document/vertex by its primary key (or
            ID), and the index is used to resolve such references.
            It is also very common to have secondary indexes. In relational databases, you can
            create several secondary indexes on the same table using the CREATE INDEX com‐
            mand, and they are often crucial for performing joins efficiently

            A secondary index can easily be constructed from a key-value index. The main differ‐
            ence is that keys are not unique; i.e., there might be many rows (documents, vertices)
            with the same key. This can be solved in two ways: either by making each value in the
            index a list of matching row identifiers (like a postings list in a full-text index) or by
            making each key unique by appending a row identifier to it. Either way, both B-trees
            and log-structured indexes can be used as secondary indexes.

            Multi-column indexes
                The indexes discussed so far only map a single key to a value. That is not sufficient if
                we need to query multiple columns of a table (or multiple fields in a document)
                simultaneously.
                The most common type of multi-column index is called a concatenated index, which
                simply combines several fields into one key by appending one column to another (the
                index definition specifies in which order the fields are concatenated). This is like an
                old-fashioned paper phone book, which provides an index from (lastname, first‐
                name) to phone number. Due to the sort order, the index can be used to find all the
                people with a particular last name, or all the people with a particular lastname
                firstname combination. However, the index is useless if you want to find all the peo‐
                ple with a particular first name

!!!!            Multi-dimensional indexes are a more general way of querying several columns at
                once, which is particularly important for geospatial data. For example, a restaurant
                search website may have a database containing the latitude and longitude of each res‐
                taurant. When a user is looking at the restaurants on a map, the website needs to
                search for all the restaurants within the rectangular map area that the user is cur‐
                rently viewing. This requires a two-dimensional range query like the following:
                    SELECT * FROM restaurants WHERE latitude > 51.4946 AND latitude < 51.5079
                     AND longitude > -0.1162 AND longitude < -0.1004;
                A standard B-tree or LSM-tree index is not able to answer that kind of query effi‐
                ciently: it can give you either all the restaurants in a range of latitudes (but at any lon‐
                gitude), or all the restaurants in a range of longitudes (but anywhere between the
                North and South poles), but not both simultaneously

            Full-text search and fuzzy indexes
                All the indexes discussed so far assume that you have exact data and allow you to
                query for exact values of a key, or a range of values of a key with a sort order. What
                they don’t allow you to do is search for similar keys, such as misspelled words. Such
                fuzzy querying requires different techniques.
                For example, full-text search engines commonly allow a search for one word to be
                expanded to include synonyms of the word, to ignore grammatical variations of
                words, and to search for occurrences of words near each other in the same document,
                and support various other features that depend on linguistic analysis of the text. To
                cope with typos in documents or queries, Lucene is able to search text for words
                within a certain edit distance (an edit distance of 1 means that one letter has been
                added, removed, or replaced)

!!!         Keeping everything in memory
                The data structures discussed so far in this chapter have all been answers to the limi‐
                tations of disks. Compared to main memory, disks are awkward to deal with. With
                both magnetic disks and SSDs, data on disk needs to be laid out carefully if you want
                good performance on reads and writes. However, we tolerate this awkwardness
                because disks have two significant advantages: they are durable (their contents are
                not lost if the power is turned off), and they have a lower cost per gigabyte than RAM.
                As RAM becomes cheaper, the cost-per-gigabyte argument is eroded. Many datasets
                are simply not that big, so it’s quite feasible to keep them entirely in memory, poten‐
                tially distributed across several machines. This has led to the development of in memory
                databases.

                Some in-memory key-value stores, such as Memcached, are intended for caching use
                only, where it’s acceptable for data to be lost if a machine is restarted. But other in
                memory databases aim for durability, which can be achieved with special hardware
                (such as battery-powered RAM), by writing a log of changes to disk, by writing peri‐
                odic snapshots to disk, or by replicating the in-memory state to other machines.

                When an in-memory database is restarted, it needs to reload its state, either from disk
                or over the network from a replica (unless special hardware is used). Despite writing
                to disk, it’s still an in-memory database, because the disk is merely used as an
                append-only log for durability, and reads are served entirely from memory. Writing
                to disk also has operational advantages: files on disk can easily be backed up,
                inspected, and analyzed by external utilities

!!!!            Products such as VoltDB, MemSQL, and Oracle TimesTen are in-memory databases
                with a relational model, and the vendors claim that they can offer big performance
                improvements by removing all the overheads associated with managing on-disk data
                structures [41, 42]. RAMCloud is an open source, in-memory key-value store with
                durability (using a log-structured approach for the data in memory as well as the data
                on disk) [43]. Redis and Couchbase provide weak durability by writing to disk asyn‐
                chronously.


    Transaction Processing or Analytics?
        Even though databases started being used for many different kinds of data—com‐
        ments on blog posts, actions in a game, contacts in an address book, etc.—the basic
        access pattern remained similar to processing business transactions. An application
        typically looks up a small number of records by some key, using an index. Records
        are inserted or updated based on the user’s input. Because these applications are
        interactive, the access pattern became known as online transaction processing
        (OLTP).
!!!!    However, databases also started being increasingly used for data analytics, which has
        very different access patterns. Usually an analytic query needs to scan over a huge
        number of records, only reading a few columns per record, and calculates aggregate
        statistics (such as count, sum, or average) rather than returning the raw data to the
        user. For example, if your data is a table of sales transactions, then analytic queries
        might be:
        • What was the total revenue of each of our stores in January?
        • How many more bananas than usual did we sell during our latest promotion?

        Dan: see "3-9 TABLE with differences between transactions vs analytics processing.png"

!!!     Nevertheless, in the late 1980s and early
        1990s, there was a trend for companies to stop using their OLTP systems for analytics
        purposes, and to run the analytics on a separate database instead. This separate data‐
        base was called a data warehouse.

        Data Warehousing
            A data warehouse, by contrast, is a separate database that analysts can query to their
            hearts’ content, without affecting OLTP operations [48]. The data warehouse con‐
            tains a read-only copy of the data in all the various OLTP systems in the company.
            Data is extracted from OLTP databases (using either a periodic data dump or a con‐
            tinuous stream of updates), transformed into an analysis-friendly schema, cleaned
            up, and then loaded into the data warehouse. This process of getting data into the
            warehouse is known as Extract–Transform–Load (ETL)

            Dan: see "3-8 data wharehousing.png"

            Data warehouses now exist in almost all large enterprises, but in small companies
            they are almost unheard of. This is probably because most small companies don’t
            have so many different OLTP systems, and most small companies have a small
            amount of data—small enough that it can be queried in a conventional SQL database,
            or even analyzed in a spreadsheet. In a large company, a lot of heavy lifting is
            required to do something that is simple in a small company.

            In the rest of this chapter we will look at storage engines that are optimized for analytics
            instead.
            The divergence between OLTP databases and data warehouses
            The data model of a data warehouse is most commonly relational, because SQL is
            generally a good fit for analytic queries. There are many graphical data analysis tools
            that generate SQL queries, visualize the results, and allow analysts to explore the data
            (through operations such as drill-down and slicing and dicing).
            On the surface, a data warehouse and a relational OLTP database look similar,
!!!         because they both have a SQL query interface. However, the internals of the systems
            can look quite different, because they are optimized for very different query patterns.
            Many database vendors now focus on supporting either transaction processing or
            analytics workloads, but not both

            Some databases, such as Microsoft SQL Server and SAP HANA, have support for
            transaction processing and data warehousing in the same product. However, they are
            increasingly becoming two separate storage and query engines, which happen to be
            accessible through a common SQL interface [49, 50, 51].
            Data warehouse vendors such as Teradata, Vertica, SAP HANA, and ParAccel typi‐
            cally sell their systems under expensive commercial licenses. Amazon RedShift is a
            hosted version of ParAccel. More recently, a plethora of open source SQL-onHadoop projects have emerged; they are young but aiming to compete with commer‐
            cial data warehouse systems. These include Apache Hive, Spark SQL, Cloudera
            Impala, Facebook Presto, Apache Tajo, and Apache Drill [52, 53]. Some of them are
            based on ideas from Google’s Dreme

        Stars and Snowflakes: Schemas for Analytics
            Many data warehouses
            are used in a fairly formulaic style, known as a star schema (also known as dimen‐
            sional modeling [55]).
            At the center of the schema is a so-called fact table (in this example,
            it is called fact_sales). Each row of the fact table represents an event that occurred
            at a particular time (here, each row represents a customer’s purchase of a product). If
            we were analyzing website traffic rather than retail sales, each row might represent a
            page view or a click by a user

!!!!!       The name “star schema” comes from the fact that when the table relationships are
            visualized, the fact table is in the middle, surrounded by its dimension tables; the
            connections to these tables are like the rays of a star.

            A variation of this template is known as the snowflake schema, where dimensions are
            further broken down into subdimensions. For example, there could be separate tables
            for brands and product categories, and each row in the dim_product table could ref‐
            erence the brand and category as foreign keys, rather than storing them as strings in
            the dim_product table. Snowflake schemas are more normalized than star schemas,
            but star schemas are often preferred because they are simpler for analysts to work
            with

                Dan: this is where https://www.snowflake.com/blog/behind-snowflakes-name/ gets its name from

    Column-Oriented Storage
        But then, a
        row-oriented storage engine still needs to load all of those rows (each consisting of
        over 100 attributes) from disk into memory, parse them, and filter out those that
        don’t meet the required conditions. That can take a long time.
        The idea behind column-oriented storage is simple: don’t store all the values from one
        row together, but store all the values from each column together instead. If each col‐
        umn is stored in a separate file, a query only needs to read and parse those columns
        that are used in that query, which can save a lot of work. This principle is illustrated
        in Figure 3-10.

        Dan: see "3-10 storing data by column not by row.png"

!!!!    The column-oriented storage layout relies on each column file containing the rows in
        the same order. Thus, if you need to reassemble an entire row, you can take the 23rd
        entry from each of the individual column files and put them together to form the
        23rd row of the table.

!!        Another advantage of sorted order is that it can help with compression of columns. If
        the primary sort column does not have many distinct values, then after sorting, it will
        have long sequences where the same value is repeated many times in a row. A simple
        run-length encoding, like we used for the bitmaps in Figure 3-11, could compress
        that column down to a few kilobytes—even if the table has billions of rows.

!!!     A clever extension of this idea was introduced in C-Store and adopted in the com‐
        mercial data warehouse Vertica [61, 62]. Different queries benefit from different sort
        orders, so why not store the same data sorted in several different ways? Data needs to
        be replicated to multiple machines anyway, so that you don’t lose data if one machine
        fails. You might as well store that redundant data sorted in different ways so that
        when you’re processing a query, you can use the version that best fits the query
        pattern.

        Writing to Column-Oriented Storage
            These optimizations make sense in data warehouses, because most of the load con‐
            sists of large read-only queries run by analysts. Column-oriented storage, compres‐
            sion, and sorting all help to make those read queries faster. However, they have the
            downside of making writes more difficult.
            An update-in-place approach, like B-trees use, is not possible with compressed col‐
            umns. If you wanted to insert a row in the middle of a sorted table, you would most
            likely have to rewrite all the column files. As rows are identified by their position
            within a column, the insertion has to update all columns consistently.
            Fortunately, we have already seen a good solution earlier in this chapter: LSM-trees.
            All writes first go to an in-memory store, where they are added to a sorted structure
            and prepared for writing to disk. It doesn’t matter whether the in-memory store is
            row-oriented or column-oriented. When enough writes have accumulated, they are
            merged with the column files on disk and written to new files in bulk. This is essen‐
            tially what Vertica does

        Aggregation: Data Cubes and Materialized Views
            Another aspect of data warehouses that is worth mentioning briefly is materialized
            aggregates. As discussed earlier, data warehouse queries often involve an aggregate
            function, such as COUNT, SUM, AVG, MIN, or MAX in SQL. If the same aggregates are used
            by many different queries, it can be wasteful to crunch through the raw data every
            time. Why not cache some of the counts or sums that queries use most often?

    As an application developer, if you’re armed with this knowledge about the internals
    of storage engines, you are in a much better position to know which tool is best suited
    for your particular application. If you need to adjust a database’s tuning parameters,
    this understanding allows you to imagine what effect a higher or a lower value may
    have.
    Although this chapter couldn’t make you an expert in tuning any one particular stor‐
    age engine, it has hopefully equipped you with enough vocabulary and ideas that you
    can make sense of the documentation for the database of your choice


========================================================================================================================
Chapter 4. Encoding and Evolution

    In most cases, a change to an application’s features also requires a change to data that
    it stores: perhaps a new field or record type needs to be captured, or perhaps existing
    data needs to be presented in a new way

    The data models we discussed in Chapter 2 have different ways of coping with such
    change. Relational databases generally assume that all data in the database conforms
    to one schema: although that schema can be changed (through schema migrations;
    i.e., ALTER statements), there is exactly one schema in force at any one point in time.
    By contrast, schema-on-read (“schemaless”) databases don’t enforce a schema, so the
    database can contain a mixture of older and newer data formats written at different
    times

    However, in a large application, code changes often cannot happen instantaneously:
        • With server-side applications you may want to perform a rolling upgrade (also
        known as a staged rollout), deploying the new version to a few nodes at a time,
        checking whether the new version is running smoothly, and gradually working
        your way through all the nodes. This allows new versions to be deployed without
        service downtime, and thus encourages more frequent releases and better evolvability.
        • With client-side applications you’re at the mercy of the user, who may not install
        the update for some time.

    This means that old and new versions of the code, and old and new data formats,
    may potentially all coexist in the system at the same time. In order for the system to
    continue running smoothly, we need to maintain compatibility in both directions:
        Backward compatibility
            Newer code can read data that was written by older code.
        Forward compatibility
            Older code can read data that was written by newer code

    Backward compatibility is normally not hard to achieve: as author of the newer code,
    you know the format of data written by older code, and so you can explicitly handle it
    (if necessary by simply keeping the old code to read the old data). Forward compati‐
    bility can be trickier, because it requires older code to ignore additions made by a
    newer version of the code.

    Formats for Encoding Data
!!!!    Programs usually work with data in (at least) two different representations:
            1. In memory, data is kept in objects, structs, lists, arrays, hash tables, trees, and so
            on. These data structures are optimized for efficient access and manipulation by
            the CPU (typically using pointers).
            2. When you want to write data to a file or send it over the network, you have to
            encode it as some kind of self-contained sequence of bytes (for example, a JSON
            document).

!!!!    Thus, we need some kind of translation between the two representations. The trans‐
        lation from the in-memory representation to a byte sequence is called encoding (also
        known as serialization or marshalling), and the reverse is called decoding (parsing,
        deserialization, unmarshalling)

        Language-Specific Formats
!!!         Many programming languages come with built-in support for encoding in-memory
            objects into byte sequences. For example, Java has java.io.Serializable [1], Ruby
            has Marshal [2], Python has pickle [3], and so on. Many third-party libraries also
            exist, such as Kryo for Java [4].

!!          These encoding libraries are very convenient, because they allow in-memory objects
            to be saved and restored with minimal additional code. However, they also have a
            number of deep problems:
                • The encoding is often tied to a particular programming language, and reading
                the data in another language is very difficult. If you store or transmit data in such
                an encoding, you are committing yourself to your current programming lan‐
                guage for potentially a very long time, and precluding integrating your systems
                with those of other organizations (which may use different languages).
                • In order to restore data in the same object types, the decoding process needs to
                be able to instantiate arbitrary classes. This is frequently a source of security
                problems [5]: if an attacker can get your application to decode an arbitrary byte
                sequence, they can instantiate arbitrary classes, which in turn often allows them
                to do terrible things such as remotely executing arbitrary code
                • Versioning data is often an afterthought in these libraries: as they are intended
                for quick and easy encoding of data, they often neglect the inconvenient prob‐
                lems of forward and backward compatibility.
                • Efficiency (CPU time taken to encode or decode, and the size of the encoded
                structure) is also often an afterthought. For example, Java’s built-in serialization
                is notorious for its bad performance and bloated encoding [8].
            For these reasons it’s generally a bad idea to use your language’s built-in encoding for
            anything other than very transient purposes.

        JSON, XML, and Binary Variants
!!!!        Moving to standardized encodings that can be written and read by many program‐
            ming languages, JSON and XML are the obvious contenders. They are widely known,
            widely supported, and almost as widely disliked. XML is often criticized for being too
            verbose and unnecessarily complicated [9]. JSON’s popularity is mainly due to its
            built-in support in web browsers (by virtue of being a subset of JavaScript) and sim‐
            plicity relative to XML. CSV is another popular language-independent format, albeit
            less powerful.

            JSON, XML, and CSV are textual formats, and thus somewhat human-readable
            (although the syntax is a popular topic of debate). Besides the superficial syntactic
            issues, they also have some subtle problems:
                • JSON and XML have good support for Unicode character strings (i.e., human readable text),
                but they don’t support binary strings (sequences of bytes without
                a character encoding). Binary strings are a useful feature, so people get around
                this limitation by encoding the binary data as text using Base64. The schema is
                then used to indicate that the value should be interpreted as Base64-encoded.
                This works, but it’s somewhat hacky and increases the data size by 33%.

            Example 4-1
                {
                 "userName": "Martin",
                 "favoriteNumber": 1337,
                 "interests": ["daydreaming", "hacking"]
                }

        Thrift and Protocol Buffers
!!!!!       Apache Thrift [15] and Protocol Buffers (protobuf) [16] are binary encoding libraries
            that are based on the same principle. Protocol Buffers was originally developed at
            Google, Thrift was originally developed at Facebook, and both were made open
            source in 2007–08

            Both Thrift and Protocol Buffers require a schema for any data that is encoded. To
            encode the data in Example 4-1 in Thrift, you would describe the schema in the
            Thrift interface definition language (IDL) like this:
                struct Person {
                 1: required string userName,
                 2: optional i64 favoriteNumber,
                 3: optional list<string> interests
                }

            The equivalent schema definition for Protocol Buffers looks very similar:
                message Person {
                 required string user_name = 1;
                 optional int64 favorite_number = 2;
                 repeated string interests = 3;
                }

!!!!        Thrift and Protocol Buffers each come with a code generation tool that takes a
            schema definition like the ones shown here, and produces classes that implement the
            schema in various programming languages [18]. Your application code can call this
            generated code to encode or decode records of the schema.

            Field tags and schema evolution
                As you can see from the examples, an encoded record is just the concatenation of its
                encoded fields. Each field is identified by its tag number (the numbers 1, 2, 3 in the
                sample schemas) and annotated with a datatype (e.g., string or integer). If a field
                value is not set, it is simply omitted from the encoded record. From this you can see
                that field tags are critical to the meaning of the encoded data. You can change the
                name of a field in the schema, since the encoded data never refers to field names, but
                you cannot change a field’s tag, since that would make all existing encoded data
                invalid.

                You can add new fields to the schema, provided that you give each field a new tag
                number. If old code (which doesn’t know about the new tag numbers you added)
                tries to read data written by new code, including a new field with a tag number it
                doesn’t recognize, it can simply ignore that field. The datatype annotation allows the
                parser to determine how many bytes it needs to skip. This maintains forward com‐
                patibility: old code can read records that were written by new code

                What about backward compatibility? As long as each field has a unique tag number,
                new code can always read old data, because the tag numbers still have the same
                meaning. The only detail is that if you add a new field, you cannot make it required.
                If you were to add a field and make it required, that check would fail if new code read
                data written by old code, because the old code will not have written the new field that
                you added. Therefore, to maintain backward compatibility, every field you add after
                the initial deployment of the schema must be optional or have a default value

        Avro
            Apache Avro [20] is another binary encoding format that is interestingly different
            from Protocol Buffers and Thrift. It was started in 2009 as a subproject of Hadoop, as
            a result of Thrift not being a good fit for Hadoop’s use cases

            Avro also uses a schema to specify the structure of the data being encoded. It has two
            schema languages: one (Avro IDL) intended for human editing, and one (based on
            JSON) that is more easily machine-readable.
            Our example schema, written in Avro IDL, might look like this:
                record Person {
                     string userName;
                     union { null, long } favoriteNumber = null;
                     array<string> interests;
                }

            To parse the binary data, you go through the fields in the order that they appear in
            the schema and use the schema to tell you the datatype of each field. This means that
            the binary data can only be decoded correctly if the code reading the data is using the
            exact same schema as the code that wrote the data. Any mismatch in the schema
            between the reader and the writer would mean incorrectly decoded data.

    Modes of Dataflow
        At the beginning of this chapter we said that whenever you want to send some data to
        another process with which you don’t share memory—for example, whenever you
        want to send data over the network or write it to a file—you need to encode it as a
        sequence of bytes. We then discussed a variety of different encodings for doing this.

        Compatibility is a relationship between one process that encodes the data, and another
        process that decodes it.

!!!!!!  That’s a fairly abstract idea—there are many ways data can flow from one process to
        another. Who encodes the data, and who decodes it? In the rest of this chapter we
        will explore some of the most common ways how data flows between processes:
        • Via databases (see “Dataflow Through Databases” on page 129)
        • Via service calls (see “Dataflow Through Services: REST and RPC” on page 131)
        • Via asynchronous message passing (see “Message-Passing Dataflow” on page 136)

        Dataflow Through Databases
            In a database, the process that writes to the database encodes the data, and the pro‐
            cess that reads from the database decodes it. There may just be a single process
            accessing the database, in which case the reader is simply a later version of the same
            process—in that case you can think of storing something in the database as sending a
            message to your future self.

            Backward compatibility is clearly necessary here; otherwise your future self won’t be
            able to decode what you previously wrote.
            In general, it’s common for several different processes to be accessing a database at
            the same time. Those processes might be several different applications or services, or
            they may simply be several instances of the same service (running in parallel for scal‐
            ability or fault tolerance). Either way, in an environment where the application is
            changing, it is likely that some processes accessing the database will be running newer
            code and some will be running older code—for example because a new version is cur‐
            rently being deployed in a rolling upgrade, so some instances have been updated
            while others haven’t yet

            This means that a value in the database may be written by a newer version of the
            code, and subsequently read by an older version of the code that is still running.
            Thus, forward compatibility is also often required for databases.

            For example, if you decode a database value into model objects in the
            application, and later reencode those model objects, the unknown field might be lost
            in that translation process. Solving this is not a hard problem; you just need to be
            aware of it.

!!!         When you deploy a new version of your application (of a server-side application, at
            least), you may entirely replace the old version with the new version within a few
            minutes. The same is not true of database contents: the five-year-old data will still be
            there, in the original encoding, unless you have explicitly rewritten it since then. This
            observation is sometimes summed up as data outlives code.
            Rewriting (migrating) data into a new schema is certainly possible, but it’s an expen‐
            sive thing to do on a large dataset, so most databases avoid it if possible. Most rela‐
            tional databases allow simple schema changes, such as adding a new column with a
            null default value, without rewriting existing data

        Dataflow Through Services: REST and RPC
            When you have processes that need to communicate over a network, there are a few
            different ways of arranging that communication. The most common arrangement is
            to have two roles: clients and servers. The servers expose an API over the network,
            and the clients can connect to the servers to make requests to that API. The API
            exposed by the server is known as a service.

            The web works this way: clients (web browsers) make requests to web servers, mak‐
            ing GET requests to download HTML, CSS, JavaScript, images, etc., and making POST
            requests to submit data to the server. The API consists of a standardized set of proto‐
            cols and data formats (HTTP, URLs, SSL/TLS, HTML, etc.). Because web browsers,
            web servers, and website authors mostly agree on these standards, you can use any
            web browser to access any website (at least in theory!).

            For example, a native app running on a
            mobile device or a desktop computer can also make network requests to a server, and
            a client-side JavaScript application running inside a web browser can use
            XMLHttpRequest to become an HTTP client (this technique is known as Ajax [30]).
            In this case, the server’s response is typically not HTML for displaying to a human,
            but rather data in an encoding that is convenient for further processing by the clientside application code (such as JSON). Although HTTP may be used as the transport
            protocol, the API implemented on top is application-specific, and the client and
            server need to agree on the details of that API.

            Moreover, a server can itself be a client to another service (for example, a typical web
            app server acts as client to a database). This approach is often used to decompose a
            large application into smaller services by area of functionality, such that one service
            makes a request to another when it requires some functionality or data from that
            other service. This way of building applications has traditionally been called a serviceoriented architecture (SOA), more recently refined and rebranded as microservices
            architecture

            There are two popular approaches to web services: REST and SOAP. They are almost
            diametrically opposed in terms of philosophy, and often the subject of heated debate
            among their respective proponents.vi
            REST is not a protocol, but rather a design philosophy that builds upon the principles
            of HTTP [34, 35]. It emphasizes simple data formats, using URLs for identifying
            resources and using HTTP features for cache control, authentication, and content
            type negotiation. REST has been gaining popularity compared to SOAP, at least in
            the context of cross-organizational service integration [36], and is often associated
            with microservices [31]. An API designed according to the principles of REST is
            called RESTful.

            By contrast, SOAP is an XML-based protocol for making network API requests.vii
            Although it is most commonly used over HTTP, it aims to be independent from
            HTTP and avoids using most HTTP features. Instead, it comes with a sprawling and
            complex multitude of related standards (the web service framework, known as WS-*)
            that add various features

            The API of a SOAP web service is described using an XML-based language called the
            Web Services Description Language, or WSDL. WSDL enables code generation so
            that a client can access a remote service using local classes and method calls (which
            are encoded to XML messages and decoded again by the framework). This is useful in
            statically typed programming languages, but less so in dynamically typed ones

            RESTful APIs tend to favor simpler approaches, typically involving less code genera‐
            tion and automated tooling. A definition format such as OpenAPI, also known as
            Swagger [40], can be used to describe RESTful APIs and produce documentation.

            The problems with remote procedure calls (RPCs)
                All of these are based on the idea of a remote procedure call (RPC), which has been
                around since the 1970s [42]. The RPC model tries to make a request to a remote net‐
                work service look the same as calling a function or method in your programming lan‐
                guage, within the same process (this abstraction is called location transparency).

            Current directions for RPC
                Despite all these problems, RPC isn’t going away. Various RPC frameworks have
                been built on top of all the encodings mentioned in this chapter: for example, Thrift
                and Avro come with RPC support included, gRPC is an RPC implementation using
                Protocol Buffers, Finagle also uses Thrift, and Rest.li uses JSON over HTTP

                gRPC supports streams, where a call consists of not
                just one request and one response, but a series of requests and responses over time

                Some of these frameworks also provide service discovery—that is, allowing a client to
                find out at which IP address and port number it can find a particular service

!!!!!           Custom RPC protocols with a binary encoding format can achieve better perfor‐
                mance than something generic like JSON over REST. However, a RESTful API has
                other significant advantages: it is good for experimentation and debugging (you can
                simply make requests to it using a web browser or the command-line tool curl,
                without any code generation or software installation), it is supported by all main‐
                stream programming languages and platforms, and there is a vast ecosystem of tools
                available (servers, caches, load balancers, proxies, firewalls, monitoring, debugging
                tools, testing tools, etc.).

!!!!            For these reasons, REST seems to be the predominant style for public APIs. The main
                focus of RPC frameworks is on requests between services owned by the same organi‐
                zation, typically within the same datacenter.

        Message-Passing Dataflow
            In this final section, we will briefly look at asynchronous message-passing systems,
            which are somewhere between RPC and databases. They are similar to RPC in that a
            client’s request (usually called a message) is delivered to another process with low
            latency. They are similar to databases in that the message is not sent via a direct net‐
            work connection, but goes via an intermediary called a message broker (also called a
            message queue or message-oriented middleware), which stores the message temporar‐
            ily

!!!!        Using a message broker has several advantages compared to direct RPC:
                • It can act as a buffer if the recipient is unavailable or overloaded, and thus
                improve system reliability.
                • It can automatically redeliver messages to a process that has crashed, and thus
                prevent messages from being lost.
                • It avoids the sender needing to know the IP address and port number of the
                recipient (which is particularly useful in a cloud deployment where virtual
                machines often come and go).
                • It allows one message to be sent to several recipients.
                • It logically decouples the sender from the recipient (the sender just publishes
                messages and doesn’t care who consumes them).

!!!!!       However, a difference compared to RPC is that message-passing communication is
            usually one-way: a sender normally doesn’t expect to receive a reply to its messages. It
            is possible for a process to send a response, but this would usually be done on a sepa‐
            rate channel. This communication pattern is asynchronous: the sender doesn’t wait
            for the message to be delivered, but simply sends it and then forgets about it.

            More recently, open source implementations such as RabbitMQ, ActiveMQ, Hor‐
            netQ, NATS, and Apache Kafka have become popular. We will compare them in
            more detail in Chapter 11.



========================================================================================================================
========================================================================================================================
========================================================================================================================
========================================================================================================================
PART 2 Distributed Data

    In Part I of this book, we discussed aspects of data systems that apply when data is
    stored on a single machine. Now, in Part II, we move up a level and ask: what hap‐
    pens if multiple machines are involved in storage and retrieval of data?

    There are various reasons why you might want to distribute a database across multiple machines:
        Scalability
        Fault tolerance/high availability
        Latency

!!  If all you need is to scale to higher load, the simplest approach is to buy a more pow‐
    erful machine (sometimes called vertical scaling or scaling up). Many CPUs, many
    RAM chips, and many disks can be joined together under one operating system, and
    a fast interconnect allows any CPU to access any part of the memory or disk.
    In this kind of shared-memory architecture, all the components can be treated as a single
    machine

    The problem with a shared-memory approach is that the cost grows faster than line‐
    arly: a machine with twice as many CPUs, twice as much RAM, and twice as much
    disk capacity as another typically costs significantly more than twice as much. And
    due to bottlenecks, a machine twice the size cannot necessarily handle twice the load

!!  By contrast, shared-nothing architectures [3] (sometimes called horizontal scaling or
    scaling out) have gained a lot of popularity. In this approach, each machine or virtual
    machine running the database software is called a node. Each node uses its CPUs,
    RAM, and disks independently. Any coordination between nodes is done at the soft‐
    ware level, using a conventional network.

    In this part of the book, we focus on shared-nothing architectures—not because they
    are necessarily the best choice for every use case, but rather because they require the
    most caution from you, the application developer. If your data is distributed across
    multiple nodes, you need to be aware of the constraints and trade-offs that occur in
    such a distributed system—the database cannot magically hide these from you.

!!! There are two common ways data is distributed across multiple nodes:
        Replication
            Keeping a copy of the same data on several different nodes, potentially in differ‐
            ent locations. Replication provides redundancy: if some nodes are unavailable,
            the data can still be served from the remaining nodes. Replication can also help
            improve performance. We discuss replication in Chapter 5.
        Partitioning
            Splitting a big database into smaller subsets called partitions so that different par‐
            titions can be assigned to different nodes (also known as sharding). We discuss
            partitioning in Chapter 6.


========================================================================================================================
Chapter 5. Replication

    Replication means keeping a copy of the same data on multiple machines that are
    connected via a network

!!! In this chapter we will assume that your dataset is so small that each machine can
    hold a copy of the entire dataset. In Chapter 6 we will relax that assumption and dis‐
    cuss partitioning (sharding) of datasets that are too big for a single machine

!!!!All of the difficulty in replication lies in handling changes to replicated data, and that’s what this
    chapter is about. We will discuss three popular algorithms for replicating changes
    between nodes: single-leader, multi-leader, and leaderless replication. Almost all dis‐
    tributed databases use one of these three approaches. They all have various pros and
    cons, which we will examine in detail.

    Leaders and Followers
        Each node that stores a copy of the database is called a replica. With multiple replicas,
        a question inevitably arises: how do we ensure that all the data ends up on all the replicas?

!!!!    Every write to the database needs to be processed by every replica; otherwise, the rep‐
        licas would no longer contain the same data. The most common solution for this is
        called leader-based replication (also known as active/passive or master–slave replica‐
        tion) and is illustrated in Figure 5-1. It works as follows:
            1. One of the replicas is designated the leader (also known as master or primary).
            When clients want to write to the database, they must send their requests to the
            leader, which first writes the new data to its local storage.
            2. The other replicas are known as followers (read replicas, slaves, secondaries, or hot
            standbys).i
             Whenever the leader writes new data to its local storage, it also sends
            the data change to all of its followers as part of a replication log or change stream.
            Each follower takes the log from the leader and updates its local copy of the data‐
            base accordingly, by applying all writes in the same order as they were processed
            on the leader.
            3. When a client wants to read from the database, it can query either the leader or
            any of the followers. However, writes are only accepted on the leader (the follow‐
            ers are read-only from the client’s point of view).

        Dan: see "5-1 leader or master-slave database replication .png"

!!!!!   This mode of replication is a built-in feature of many relational databases, such as
        PostgreSQL (since version 9.0), MySQL, Oracle Data Guard [2], and SQL Server’s
        AlwaysOn Availability Groups [3]. It is also used in some nonrelational databases,
        including MongoDB, RethinkDB, and Espresso [4]. Finally, leader-based replication
        is not restricted to only databases: distributed message brokers such as Kafka [5] and
        RabbitMQ highly available queues [6] also use it.

        Synchronous Versus Asynchronous Replication
            An important detail of a replicated system is whether the replication happens syn‐
            chronously or asynchronously. (In relational databases, this is often a configurable
            option; other systems are often hardcoded to be either one or the other.)

            The advantage of synchronous replication is that the follower is guaranteed to have
            an up-to-date copy of the data that is consistent with the leader. If the leader sud‐
            denly fails, we can be sure that the data is still available on the follower. The disad‐
            vantage is that if the synchronous follower doesn’t respond (because it has crashed,
            or there is a network fault, or for any other reason), the write cannot be processed.
            The leader must block all writes and wait until the synchronous replica is available
            again.

!!!!!       For that reason, it is impractical for all followers to be synchronous: any one node
            outage would cause the whole system to grind to a halt. In practice, if you enable syn‐
            chronous replication on a database, it usually means that one of the followers is syn‐
            chronous, and the others are asynchronous. If the synchronous follower becomes
            unavailable or slow, one of the asynchronous followers is made synchronous. This
            guarantees that you have an up-to-date copy of the data on at least two nodes: the
            leader and one synchronous follower. This configuration is sometimes also called
            semi-synchronous

            Often, leader-based replication is configured to be completely asynchronous. In this
            case, if the leader fails and is not recoverable, any writes that have not yet been repli‐
            cated to followers are lost. This means that a write is not guaranteed to be durable,
            even if it has been confirmed to the client. However, a fully asynchronous configura‐
            tion has the advantage that the leader can continue processing writes, even if all of its
            followers have fallen behind

        Setting Up New Followers
            Simply copying data files from one node to another is typically not sufficient: clients
            are constantly writing to the database, and the data is always in flux, so a standard file
            copy would see different parts of the database at different points in time. The result
            might not make any sense.
            You could make the files on disk consistent by locking the database (making it
            unavailable for writes), but that would go against our goal of high availability. Fortu‐
            nately, setting up a follower can usually be done without downtime. Conceptually,
            the process looks like this:
                1. Take a consistent snapshot of the leader’s database at some point in time—if pos‐
                sible, without taking a lock on the entire database. Most databases have this fea‐
                ture, as it is also required for backups. In some cases, third-party tools are
                needed, such as innobackupex for MySQL [12].
                2. Copy the snapshot to the new follower node.
                3. The follower connects to the leader and requests all the data changes that have
                happened since the snapshot was taken. This requires that the snapshot is associ‐
                ated with an exact position in the leader’s replication log. That position has vari‐
                ous names: for example, PostgreSQL calls it the log sequence number, and
                MySQL calls it the binlog coordinates.
                4. When the follower has processed the backlog of data changes since the snapshot,
                we say it has caught up. It can now continue to process data changes from the
                leader as they happen.

!!!         Handling a failure of the leader is trickier: one of the followers needs to be promoted
            to be the new leader, clients need to be reconfigured to send their writes to the new
            leader, and the other followers need to start consuming data changes from the new
            leader. This process is called failover.
            An automatic failover process usually consists of the following steps:
                1. Determining that the leader has failed. There are many things that could poten‐
                tially go wrong: crashes, power outages, network issues, and more. There is no
                foolproof way of detecting what has gone wrong, so most systems simply use a
                timeout: nodes frequently bounce messages back and forth between each other,
                and if a node doesn’t respond for some period of time—say, 30 seconds—it is
                assumed to be dead. (If the leader is deliberately taken down for planned mainte‐
                nance, this doesn’t apply.)
                2. Choosing a new leader. This could be done through an election process (where
                the leader is chosen by a majority of the remaining replicas), or a new leader
                could be appointed by a previously elected controller node. The best candidate for
                leadership is usually the replica with the most up-to-date data changes from the
                old leader (to minimize any data loss). Getting all the nodes to agree on a new
                leader is a consensus problem, discussed in detail in Chapter 9.
                3. Reconfiguring the system to use the new leader. Clients now need to send
                their write requests to the new leader (we discuss this in “Request Routing” on
                page 214). If the old leader comes back, it might still believe that it is the leader,
                not realizing that the other replicas have forced it to step down. The system
                needs to ensure that the old leader becomes a follower and recognizes the new
                leader

    Problems with Replication Lag
!!!!    Leader-based replication requires all writes to go through a single node, but read only queries
        can go to any replica. For workloads that consist of mostly reads and
        only a small percentage of writes (a common pattern on the web), there is an attrac‐
        tive option: create many followers, and distribute the read requests across those fol‐
        lowers. This removes load from the leader and allows read requests to be served by
        nearby replicas.
        In this read-scaling architecture, you can increase the capacity for serving read-only
        requests simply by adding more followers. However, this approach only realistically
        works with asynchronous replication—if you tried to synchronously replicate to all
        followers, a single node failure or network outage would make the entire system
        unavailable for writing
!!!        And the more nodes you have, the likelier it is that one will
        be down, so a fully synchronous configuration would be very unreliable.
        Unfortunately, if an application reads from an asynchronous follower, it may see out‐
        dated information if the follower has fallen behind. This leads to apparent inconsis‐
        tencies in the database: if you run the same query on the leader and a follower at the
        same time, you may get different results, because not all writes have been reflected in
        the follower. This inconsistency is just a temporary state—if you stop writing to the
        database and wait a while, the followers will eventually catch up and become consis‐
        tent with the leader. For that reason, this effect is known as eventual consistency

        The term “eventually” is deliberately vague: in general, there is no limit to how far a
        replica can fall behind. In normal operation, the delay between a write happening on
        the leader and being reflected on a follower—the replication lag—may be only a frac‐
        tion of a second, and not noticeable in practice. However, if the system is operating
        near capacity or if there is a problem in the network, the lag can easily increase to
        several seconds or even minutes.

        Reading Your Own Writes
            Many applications let the user submit some data and then view what they have sub‐
            mitted. This might be a record in a customer database, or a comment on a discussion
            thread, or something else of that sort. When new data is submitted, it must be sent to
            the leader, but when the user views the data, it can be read from a follower. This is
            especially appropriate if data is frequently viewed but only occasionally written.
            With asynchronous replication, there is a problem, illustrated in Figure 5-3: if the
            user views the data shortly after making a write, the new data may not yet have
            reached the replica. To the user, it looks as though the data they submitted was lost,
            so they will be understandably unhappy.

!!!         In this situation, we need read-after-write consistency, also known as read-your-writes
            consistency [24]. This is a guarantee that if the user reloads the page, they will always
            see any updates they submitted themselves. It makes no promises about other users:
            other users’ updates may not be visible until some later time. However, it reassures
            the user that their own input has been saved correctly

            How can we implement read-after-write consistency in a system with leader-based
            replication? There are various possible techniques. To mention a few:
                • When reading something that the user may have modified, read it from the
                leader; otherwise, read it from a follower. This requires that you have some way
                of knowing whether something might have been modified, without actually
                querying it. For example, user profile information on a social network is nor‐
                mally only editable by the owner of the profile, not by anybody else. Thus, a sim‐
                ple rule is: always read the user’s own profile from the leader, and any other
                users’ profiles from a follower.
                • If most things in the application are potentially editable by the user, that
                approach won’t be effective, as most things would have to be read from the
                leader (negating the benefit of read scaling). In that case, other criteria may be
                used to decide whether to read from the leader. For example, you could track the
                time of the last update and, for one minute after the last update, make all reads
                from the leader. You could also monitor the replication lag on followers and pre‐
                vent queries on any follower that is more than one minute behind the leader.
                • The client can remember the timestamp of its most recent write—then the sys‐
                tem can ensure that the replica serving any reads for that user reflects updates at
                least until that timestamp. If a replica is not sufficiently up to date, either the read
                can be handled by another replica or the query can wait until the replica has
                caught up. The timestamp could be a logical timestamp (something that indicates
                ordering of writes, such as the log sequence number) or the actual system clock
                (in which case clock synchronization becomes critical;

            Another complication arises when the same user is accessing your service from mul‐
            tiple devices, for example a desktop web browser and a mobile app. In this case you
            may want to provide cross-device read-after-write consistency: if the user enters some
            information on one device and then views it on another device, they should see the
            information they just entered.

        Monotonic Reads
            Our second example of an anomaly that can occur when reading from asynchronous
            followers is that it’s possible for a user to see things moving backward in time.
            This can happen if a user makes several reads from different replicas. For example,
            Figure 5-4 shows user 2345 making the same query twice, first to a follower with little
            lag, then to a follower with greater lag. (This scenario is quite likely if the user
            refreshes a web page, and each request is routed to a random server.) The first query
            returns a comment that was recently added by user 1234, but the second query
            doesn’t return anything because the lagging follower has not yet picked up that write.
            In effect, the second query is observing the system at an earlier point in time than the
            first query. This wouldn’t be so bad if the first query hadn’t returned anything,
            because user 2345 probably wouldn’t know that user 1234 had recently added a com‐
            ment. However, it’s very confusing for user 2345 if they first see user 1234’s comment
            appear, and then see it disappear again.

!!!!!       Monotonic reads [23] is a guarantee that this kind of anomaly does not happen. It’s a
            lesser guarantee than strong consistency, but a stronger guarantee than eventual con‐
            sistency. When you read data, you may see an old value; monotonic reads only means
            that if one user makes several reads in sequence, they will not see time go backward—
            i.e., they will not read older data after having previously read newer data.
            One way of achieving monotonic reads is to make sure that each user always makes
            their reads from the same replica (different users can read from different replicas).
            For example, the replica can be chosen based on a hash of the user ID, rather than
            randomly. However, if that replica fails, the user’s queries will need to be rerouted to
            another replica.

        Consistent Prefix Reads
            Our third example of replication lag anomalies concerns violation of causality. Imag‐
            ine the following short dialog between Mr. Poons and Mrs. Cake:
                Mr. Poons
                    How far into the future can you see, Mrs. Cake?
                Mrs. Cake
                    About ten seconds usually, Mr. Poons.
            There is a causal dependency between those two sentences: Mrs. Cake heard Mr.
            Poons’s question and answered it.
            Now, imagine a third person is listening to this conversation through followers. The
            things said by Mrs. Cake go through a follower with little lag, but the things said by
            Mr. Poons have a longer replication lag (see Figure 5-5). This observer would hear
            the following:
                Mrs. Cake
                    About ten seconds usually, Mr. Poons.
                Mr. Poons
                    How far into the future can you see, Mrs. Cake?
            To the observer it looks as though Mrs. Cake is answering the question before Mr.
            Poons has even asked it.

    Multi-Leader Replication
        So far in this chapter we have only considered replication architectures using a single
        leader. Although that is a common approach, there are interesting alternatives.
!!!!    Leader-based replication has one major downside: there is only one leader, and all
        writes must go through it.iv If you can’t connect to the leader for any reason, for
        example due to a network interruption between you and the leader, you can’t write to
        the database.
        A natural extension of the leader-based replication model is to allow more than one
        node to accept writes. Replication still happens in the same way: each node that pro‐
        cesses a write must forward that data change to all the other nodes. We call this a
        multi-leader configuration (also known as master–master or active/active replication).
        In this setup, each leader simultaneously acts as a follower to the other leaders.

        Use Cases for Multi-Leader Replication
            Multi-datacenter operation
!!!!!           Imagine you have a database with replicas in several different datacenters (perhaps so
                that you can tolerate failure of an entire datacenter, or perhaps in order to be closer
                to your users). With a normal leader-based replication setup, the leader has to be in
                one of the datacenters, and all writes must go through that datacenter.
                In a multi-leader configuration, you can have a leader in each datacenter. Figure 5-6
                shows what this architecture might look like. Within each datacenter, regular leader–
                follower replication is used; between datacenters, each datacenter’s leader replicates
                its changes to the leaders in other datacenters.

                Dan: see "5-6 multi leader replication across data centers.png"

                Some databases support multi-leader configurations by default, but it is also often
                implemented with external tools, such as Tungsten Replicator for MySQL [26], BDR
                for PostgreSQL [27], and GoldenGate for Oracle

                Although multi-leader replication has advantages, it also has a big downside: the
                same data may be concurrently modified in two different datacenters, and those write
                conflicts must be resolved (indicated as “conflict resolution” in Figure 5-6).

            Clients with offline operation
                Another situation in which multi-leader replication is appropriate is if you have an
                application that needs to continue to work while it is disconnected from the internet.
                For example, consider the calendar apps on your mobile phone, your laptop, and
                other devices. You need to be able to see your meetings (make read requests) and
                enter new meetings (make write requests) at any time, regardless of whether your
                device currently has an internet connection. If you make any changes while you are
                offline, they need to be synced with a server and your other devices when the device
                is next online.

                There are tools that aim to make this kind of multi-leader configuration easier. For
                example, CouchDB is designed for this mode of operation

            Collaborative editing
                Real-time collaborative editing applications allow several people to edit a document
                simultaneously. For example, Etherpad [30] and Google Docs [31] allow multiple
                people to concurrently edit a text document or spreadsheet

        Handling Write Conflicts
            The biggest problem with multi-leader replication is that write conflicts can occur,
            which means that conflict resolution is required.

            For example, consider a wiki page that is simultaneously being edited by two users, as
            shown in Figure 5-7. User 1 changes the title of the page from A to B, and user 2
            changes the title from A to C at the same time. Each user’s change is successfully
            applied to their local leader. However, when the changes are asynchronously replica‐
            ted, a conflict is detected [33]. This problem does not occur in a single-leader data‐
            base

            Conflict avoidance
                The simplest strategy for dealing with conflicts is to avoid them: if the application can
                ensure that all writes for a particular record go through the same leader, then con‐
                flicts cannot occur. Since many implementations of multi-leader replication handle
                conflicts quite poorly, avoiding conflicts is a frequently recommended approach

                For example, in an application where a user can edit their own data, you can ensure
                that requests from a particular user are always routed to the same datacenter and use
                the leader in that datacenter for reading and writing. Different users may have differ‐
                ent “home” datacenters (perhaps picked based on geographic proximity to the user),
                but from any one user’s point of view the configuration is essentially single-leader.

!!!!!!          There are various ways of achieving convergent conflict resolution:
                    • Give each write a unique ID (e.g., a timestamp, a long random number, a UUID,
                    or a hash of the key and value), pick the write with the highest ID as the winner,
                    and throw away the other writes. If a timestamp is used, this technique is known
                    as last write wins (LWW). Although this approach is popular, it is dangerously
                    prone to data loss [35]. We will discuss LWW in more detail at the end of this
                    chapter (“Detecting Concurrent Writes” on page 184).
                    • Give each replica a unique ID, and let writes that originated at a higher
                    numbered replica always take precedence over writes that originated at a lower
                    numbered replica. This approach also implies data loss.
                    • Somehow merge the values together—e.g., order them alphabetically and then
                    concatenate them (in Figure 5-7, the merged title might be something like
                    “B/C”).
                    • Record the conflict in an explicit data structure that preserves all information,
                    and write application code that resolves the conflict at some later time (perhaps
                    by prompting the user).

            Custom conflict resolution logic
                As the most appropriate way of resolving a conflict may depend on the application,
                most multi-leader replication tools let you write conflict resolution logic using appli‐
                cation code. That code may be executed on write or on read:
                    On write
                        As soon as the database system detects a conflict in the log of replicated changes,
                        it calls the conflict handler. For example, Bucardo allows you to write a snippet of
                        Perl for this purpose. This handler typically cannot prompt a user—it runs in a
                        background process and it must execute quickly.
                    On read
                        When a conflict is detected, all the conflicting writes are stored. The next time
                        the data is read, these multiple versions of the data are returned to the applica‐
                        tion. The application may prompt the user or automatically resolve the conflict,
                        and write the result back to the database. CouchDB works this way, for example.

        Multi-Leader Replication Topologies
            A replication topology describes the communication paths along which writes are
            propagated from one node to another. If you have two leaders, like in Figure 5-7,
            there is only one plausible topology: leader 1 must send all of its writes to leader 2,
            and vice versa. With more than two leaders, various different topologies are possible.
            Some examples are illustrated in Figure 5-8.

            Dan: see "5-8 multi leader replication topologies.png"

            The most general topology is all-to-all (Figure 5-8 [c]), in which every leader sends its
            writes to every other leader. However, more restricted topologies are also used: for
            example, MySQL by default supports only a circular topology [34], in which each
            node receives writes from one node and forwards those writes (plus any writes of its
            own) to one other node. Another popular topology has the shape of a star:
            one designated root node forwards writes to all of the other nodes. The star topology can be
            generalized to a tree.

!!!!        A problem with circular and star topologies is that if just one node fails, it can inter‐
            rupt the flow of replication messages between other nodes, causing them to be unable
            to communicate until the node is fixed. The topology could be reconfigured to work
            around the failed node, but in most deployments such reconfiguration would have to
            be done manually. The fault tolerance of a more densely connected topology (such as
            all-to-all) is better because it allows messages to travel along different paths, avoiding
            a single point of failure

    Leaderless Replication
        The replication approaches we have discussed so far in this chapter—single-leader
        and multi-leader replication—are based on the idea that a client sends a write request
        to one node (the leader), and the database system takes care of copying that write to
        the other replicas. A leader determines the order in which writes should be processed,
        and followers apply the leader’s writes in the same order.

        Some data storage systems take a different approach, abandoning the concept of a
        leader and allowing any replica to directly accept writes from clients. Some of the ear‐
        liest replicated data systems were leaderless, but the idea was mostly forgotten
        during the era of dominance of relational databases. It once again became a fashiona‐
        ble architecture for databases after Amazon used it for its in-house Dynamo system.
        vi Riak, Cassandra, and Voldemort are open source datastores with leaderless
        replication models inspired by Dynamo, so this kind of database is also known as
        Dynamo-style.

        In some leaderless implementations, the client directly sends its writes to several rep‐
        licas, while in others, a coordinator node does this on behalf of the client

        Writing to the Database When a Node Is Down
            Imagine you have a database with three replicas, and one of the replicas is currently
            unavailable—perhaps it is being rebooted to install a system update. In a leader-based
            configuration, if you want to continue processing writes, you may need to perform a
            failover

            On the other hand, in a leaderless configuration, failover does not exist. Figure 5-10
            shows what happens: the client (user 1234) sends the write to all three replicas in par‐
            allel, and the two available replicas accept the write but the unavailable replica misses
            it. Let’s say that it’s sufficient for two out of three replicas to acknowledge the write:
            after user 1234 has received two ok responses, we consider the write to be successful.
            The client simply ignores the fact that one of the replicas missed the write

            Now imagine that the unavailable node comes back online, and clients start reading
            from it. Any writes that happened while the node was down are missing from that
            node. Thus, if you read from that node, you may get stale (outdated) values as
            responses.
!!!!        To solve that problem, when a client reads from the database, it doesn’t just send its
            request to one replica: read requests are also sent to several nodes in parallel. The cli‐
            ent may get different responses from different nodes; i.e., the up-to-date value from
            one node and a stale value from another. Version numbers are used to determine
            which value is newer


========================================================================================================================
Chapter 6. Partitioning

!!! In Chapter 5 we discussed replication—that is, having multiple copies of the same
    data on different nodes. For very large datasets, or very high query throughput, that is
    not sufficient: we need to break the data up into partitions, also known as sharding.

!!! Terminological confusion
    What we call a partition here is called a shard in MongoDB, Elas‐
    ticsearch, and SolrCloud; it’s known as a region in HBase, a tablet
    in Bigtable, a vnode in Cassandra and Riak, and a vBucket in
    Couchbase. However, partitioning is the most established term, so
    we’ll stick with that.

    Normally, partitions are defined in such a way that each piece of data (each record,
    row, or document) belongs to exactly one partition. There are various ways of achiev‐
    ing this, which we discuss in depth in this chapter. In effect, each partition is a small
    database of its own, although the database may support operations that touch multi‐
    ple partitions at the same time.
    The main reason for wanting to partition data is scalability

    Partitioning and Replication
        Partitioning is usually combined with replication so that copies of each partition are
        stored on multiple nodes. This means that, even though each record belongs to
        exactly one partition, it may still be stored on several different nodes for fault toler‐
        ance.

    Partitioning of Key-Value Data
        Say you have a large amount of data, and you want to partition it. How do you decide
        which records to store on which nodes?
        Our goal with partitioning is to spread the data and the query load evenly across
        nodes. If every node takes a fair share, then—in theory—10 nodes should be able to
        handle 10 times as much data and 10 times the read and write throughput of a single
        node (ignoring replication for now).

        If the partitioning is unfair, so that some partitions have more data or queries than
        others, we call it skewed. The presence of skew makes partitioning much less effective.
        In an extreme case, all the load could end up on one partition, so 9 out of 10 nodes
        are idle and your bottleneck is the single busy node. A partition with disproportion‐
        ately high load is called a hot spot.

!!!     Partitioning by Key Range
            One way of partitioning is to assign a continuous range of keys (from some mini‐
            mum to some maximum) to each partition, like the volumes of a paper encyclopedia
            (Figure 6-2). If you know the boundaries between the ranges, you can easily deter‐
            mine which partition contains a given key. If you also know which partition is
            assigned to which node, then you can make your request directly to the appropriate
            node (or, in the case of the encyclopedia, pick the correct book off the shelf).

            Dan see "6-2 partition by key range"

            The ranges of keys are not necessarily evenly spaced, because your data may not be
            evenly distributed. For example, in Figure 6-2, volume 1 contains words starting with
            A and B, but volume 12 contains words starting with T, U, V, X, Y, and Z. Simply
            having one volume per two letters of the alphabet would lead to some volumes being
            much bigger than others. In order to distribute the data evenly, the partition bound‐
            aries need to adapt to the data.
            The partition boundaries might be chosen manually by an administrator, or the data‐
            base can choose them automatically (we will discuss choices of partition boundaries
            in more detail in “Rebalancing Partitions” on page 209). This partitioning strategy is
            used by Bigtable, its open source equivalent HBase [2, 3], RethinkDB, and MongoDB
            before version 2.4

            Within each partition, we can keep keys in sorted order (see “SSTables and LSMTrees” on page 76).
            This has the advantage that range scans are easy, and you can
            treat the key as a concatenated index in order to fetch several related records in one
            query (see “Multi-column indexes” on page 87). For example, consider an application
            that stores data from a network of sensors, where the key is the timestamp of the
            measurement (year-month-day-hour-minute-second). Range scans are very useful in
            this case, because they let you easily fetch, say, all the readings from a particular
            month.

            However, the downside of key range partitioning is that certain access patterns can
            lead to hot spots. If the key is a timestamp, then the partitions correspond to ranges
            of time—e.g., one partition per day. Unfortunately, because we write data from the
            sensors to the database as the measurements happen, all the writes end up going to
            the same partition (the one for today), so that partition can be overloaded with writes
            while others sit idle

            To avoid this problem in the sensor database, you need to use something other than
            the timestamp as the first element of the key. For example, you could prefix each
            timestamp with the sensor name so that the partitioning is first by sensor name and
            then by time. Assuming you have many sensors active at the same time, the write
            load will end up more evenly spread across the partitions. Now, when you want to
            fetch the values of multiple sensors within a time range, you need to perform a sepa‐
            rate range query for each sensor name.

        Partitioning by Hash of Key
            Because of this risk of skew and hot spots, many distributed datastores use a hash
            function to determine the partition for a given key.
            A good hash function takes skewed data and makes it uniformly distributed. Say you
            have a 32-bit hash function that takes a string. Whenever you give it a new string, it
            returns a seemingly random number between 0 and 2^32 − 1. Even if the input strings
            are very similar, their hashes are evenly distributed across that range of numbers.
            For partitioning purposes, the hash function need not be cryptographically strong:
            for example, Cassandra and MongoDB use MD5, and Voldemort uses the Fowler–
            Noll–Vo function. Many programming languages have simple hash functions built in
            (as they are used for hash tables), but they may not be suitable for partitioning: for
            example, in Java’s Object.hashCode() and Ruby’s Object#hash, the same key may
            have a different hash value in different processes.
            Once you have a suitable hash function for keys, you can assign each partition a
            range of hashes (rather than a range of keys), and every key whose hash falls within a
            partition’s range will be stored in that partition

            Unfortunately however, by using the hash of the key for partitioning we lose a nice
            property of key-range partitioning: the ability to do efficient range queries. Keys that
            were once adjacent are now scattered across all the partitions, so their sort order is
            lost. In MongoDB, if you have enabled hash-based sharding mode, any range query
            has to be sent to all partitions [4]. Range queries on the primary key are not sup‐
            ported by Riak [9], Couchbase [10], or Voldemort.

        Skewed Workloads and Relieving Hot Spots
            As discussed, hashing a key to determine its partition can help reduce hot spots.
            However, it can’t avoid them entirely: in the extreme case where all reads and writes
            are for the same key, you still end up with all requests being routed to the same partition.
            This kind of workload is perhaps unusual, but not unheard of: for example, on a
            social media site, a celebrity user with millions of followers may cause a storm of
            activity when they do something [14]. This event can result in a large volume of
            writes to the same key (where the key is perhaps the user ID of the celebrity, or the ID
            of the action that people are commenting on).

            Today, most data systems are not able to automatically compensate for such a highly
            skewed workload, so it’s the responsibility of the application to reduce the skew. For
            example, if one key is known to be very hot, a simple technique is to add a random
            number to the beginning or end of the key. Just a two-digit decimal random number
            would split the writes to the key evenly across 100 different keys, allowing those keys
            to be distributed to different partitions.
            However, having split the writes across different keys, any reads now have to do addi‐
            tional work, as they have to read the data from all 100 keys and combine it. This tech‐
            nique also requires additional bookkeeping: it only makes sense to append the
            random number for the small number of hot keys; for the vast majority of keys with
            low write throughput this would be unnecessary overhead. Thus, you also need some
            way of keeping track of which keys are being split.


    Partitioning and Secondary Indexes
        The partitioning schemes we have discussed so far rely on a key-value data model. If
        records are only ever accessed via their primary key, we can determine the partition
        from that key and use it to route read and write requests to the partition responsible
        for that key.

        The situation becomes more complicated if secondary indexes are involved (see also
        “Other Indexing Structures” on page 85). A secondary index usually doesn’t identify
        a record uniquely but rather is a way of searching for occurrences of a particular
        value: find all actions by user 123, find all articles containing the word hogwash, find
        all cars whose color is red, and so on.
        Secondary indexes are the bread and butter of relational databases, and they are com‐
        mon in document databases too. Many key-value stores (such as HBase and Volde‐
        mort) have avoided secondary indexes because of their added implementation
        complexity, but some (such as Riak) have started adding them because they are so
        useful for data modeling

        The problem with secondary indexes is that they don’t map neatly to partitions.
        There are two main approaches to partitioning a database with secondary indexes:
        document-based partitioning and term-based partitioning.

        Partitioning Secondary Indexes by Document
            For example, imagine you are operating a website for selling used cars (illustrated in
            Figure 6-4). Each listing has a unique ID—call it the document ID—and you partition
            the database by the document ID (for example, IDs 0 to 499 in partition 0, IDs 500 to
            999 in partition 1, etc.).
            You want to let users search for cars, allowing them to filter by color and by make, so
            you need a secondary index on color and make (in a document database these would
            be fields; in a relational database they would be columns). If you have declared the
            index, the database can perform the indexing automatically. For example, whenever
            a red car is added to the database, the database partition automatically adds it to the
            list of document IDs for the index entry color:red.

            However, reading from a document-partitioned index requires care: unless you have
            done something special with the document IDs, there is no reason why all the cars
            with a particular color or a particular make would be in the same partition. In
            Figure 6-4, red cars appear in both partition 0 and partition 1. Thus, if you want to
            search for red cars, you need to send the query to all partitions, and combine all the
            results you get back

        Partitioning Secondary Indexes by Term
            Rather than each partition having its own secondary index (a local index), we can
            construct a global index that covers data in all partitions. However, we can’t just store
            that index on one node, since it would likely become a bottleneck and defeat the pur‐
            pose of partitioning. A global index must also be partitioned, but it can be partitioned
            differently from the primary key index.

            The advantage of a global (term-partitioned) index over a document-partitioned
            index is that it can make reads more efficient: rather than doing scatter/gather over
            all partitions, a client only needs to make a request to the partition containing the
            term that it wants. However, the downside of a global index is that writes are slower
            and more complicated, because a write to a single document may now affect multiple
            partitions of the index (every term in the document might be on a different partition,
            on a different node)

            In an ideal world, the index would always be up to date, and every document written
            to the database would immediately be reflected in the index. However, in a term partitioned
            index, that would require a distributed transaction across all partitions
            affected by a write, which is not supported in all databases

            In practice, updates to global secondary indexes are often asynchronous (that is, if
            you read the index shortly after a write, the change you just made may not yet be
            reflected in the index). For example, Amazon DynamoDB states that its global secon‐
            dary indexes are updated within a fraction of a second in normal circumstances, but
            may experience longer propagation delays in cases of faults in the infrastructure

    Rebalancing Partitions
        Over time, things change in a database:
            • The query throughput increases, so you want to add more CPUs to handle the
            load.
            • The dataset size increases, so you want to add more disks and RAM to store it.
            • A machine fails, and other machines need to take over the failed machine’s
            responsibilities.
            All of these changes call for data and requests to be moved from one node to another.
            The process of moving load from one node in the cluster to another is called reba‐
            lancing.

        No matter which partitioning scheme is used, rebalancing is usually expected to meet
        some minimum requirements:
            • After rebalancing, the load (data storage, read and write requests) should be
            shared fairly between the nodes in the cluster.
            • While rebalancing is happening, the database should continue accepting reads
            and writes.
            • No more data than necessary should be moved between nodes, to make rebalanc‐
            ing fast and to minimize the network and disk I/O load

        Fixed number of partitions
            Fortunately, there is a fairly simple solution: create many more partitions than there
            are nodes, and assign several partitions to each node. For example, a database run‐
            ning on a cluster of 10 nodes may be split into 1,000 partitions from the outset so that
            approximately 100 partitions are assigned to each node.
            Now, if a node is added to the cluster, the new node can steal a few partitions from
            every existing node until partitions are fairly distributed once again. This process is
            illustrated in Figure 6-6. If a node is removed from the cluster, the same happens in
            reverse.
            Only entire partitions are moved between nodes. The number of partitions does not
            change, nor does the assignment of keys to partitions. The only thing that changes is
            the assignment of partitions to nodes. This change of assignment is not immediate—
            it takes some time to transfer a large amount of data over the network—so the old
            assignment of partitions is used for any reads and writes that happen while the trans‐
            fer is in progress.

        Dynamic partitioning
            For databases that use key range partitioning (see “Partitioning by Key Range” on
            page 202), a fixed number of partitions with fixed boundaries would be very incon‐
            venient: if you got the boundaries wrong, you could end up with all of the data in one
            partition and all of the other partitions empty. Reconfiguring the partition bound‐
            aries manually would be very tedious.
            For that reason, key range–partitioned databases such as HBase and RethinkDB cre‐
            ate partitions dynamically. When a partition grows to exceed a configured size (on
            HBase, the default is 10 GB), it is split into two partitions so that approximately half
            of the data ends up on each side of the split [26]. Conversely, if lots of data is deleted
            and a partition shrinks below some threshold, it can be merged with an adjacent par‐
            tition.

            An advantage of dynamic partitioning is that the number of partitions adapts to the
            total data volume. If there is only a small amount of data, a small number of parti‐
            tions is sufficient, so overheads are small; if there is a huge amount of data, the size of
            each individual partition is limited to a configurable maximum

        Operations: Automatic or Manual Rebalancing
            There is one important question with regard to rebalancing that we have glossed
            over: does the rebalancing happen automatically or manually?
            There is a gradient between fully automatic rebalancing (the system decides automat‐
            ically when to move partitions from one node to another, without any administrator
            interaction) and fully manual (the assignment of partitions to nodes is explicitly con‐
            figured by an administrator, and only changes when the administrator explicitly
            reconfigures it). For example, Couchbase, Riak, and Voldemort generate a suggested
            partition assignment automatically, but require an administrator to commit it before
            it takes effect.
            Fully automated rebalancing can be convenient, because there is less operational
            work to do for normal maintenance. However, it can be unpredictable. Rebalancing
            is an expensive operation, because it requires rerouting requests and moving a large
            amount of data from one node to another. If it is not done carefully, this process can
            overload the network or the nodes and harm the performance of other requests while
            the rebalancing is in progress.

    Request Routing
        We have now partitioned our dataset across multiple nodes running on multiple
        machines. But there remains an open question: when a client wants to make a
        request, how does it know which node to connect to? As partitions are rebalanced,
        the assignment of partitions to nodes changes. Somebody needs to stay on top of
        those changes in order to answer the question: if I want to read or write the key “foo”,
        which IP address and port number do I need to connect to?

        This is an instance of a more general problem called service discovery, which isn’t
        limited to just databases. Any piece of software that is accessible over a network has
        this problem, especially if it is aiming for high availability (running in a redundant
        configuration on multiple machines). Many companies have written their own inhouse service discovery tools, and
        many of these have been released as open source

        On a high level, there are a few different approaches to this problem:
            1. Allow clients to contact any node (e.g., via a round-robin load balancer). If that
            node coincidentally owns the partition to which the request applies, it can handle
            the request directly; otherwise, it forwards the request to the appropriate node,
            receives the reply, and passes the reply along to the client.
            2. Send all requests from clients to a routing tier first, which determines the node
            that should handle each request and forwards it accordingly. This routing tier
            does not itself handle any requests; it only acts as a partition-aware load balancer.
            3. Require that clients be aware of the partitioning and the assignment of partitions
            to nodes. In this case, a client can connect directly to the appropriate node,
            without any intermediary.
        In all cases, the key problem is: how does the component making the routing decision
        (which may be one of the nodes, or the routing tier, or the client) learn about changes
        in the assignment of partitions to nodes?

        Dan : see "6-8 using zookeeper to track the assignment of partitions to nodes"

        Many distributed data systems rely on a separate coordination service such as Zoo‐
        Keeper to keep track of this cluster metadata, as illustrated in Figure 6-8. Each node
        registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of
        partitions to nodes. Other actors, such as the routing tier or the partitioning-aware
        client, can subscribe to this information in ZooKeeper. Whenever a partition changes
        ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that
        it can keep its routing information up to date.


========================================================================================================================
Chapter 7. Transactions

!!  For decades, transactions have been the mechanism of choice for simplifying these
    issues. A transaction is a way for an application to group several reads and writes
    together into a logical unit. Conceptually, all the reads and writes in a transaction are
    executed as one operation: either the entire transaction succeeds (commit) or it fails
    (abort, rollback). If it fails, the application can safely retry. With transactions, error
    handling becomes much simpler for an application, because it doesn’t need to worry
    about partial failure—i.e., the case where some operations succeed and some fail (for
    whatever reason)

!!! Not every application needs transactions, and sometimes there are advantages to
    weakening transactional guarantees or abandoning them entirely (for example, to
    achieve higher performance or higher availability). Some safety properties can be
    achieved without transactions.
    How do you figure out whether you need transactions? In order to answer that ques‐
    tion, we first need to understand exactly what safety guarantees transactions can pro‐
    vide, and what costs are associated with them. Although transactions seem
    straightforward at first glance, there are actually many subtle but important details
    that come into play

    This chapter applies to both single-node and distributed databases;

    The Slippery Concept of a Transaction
!!!!    In the late 2000s, nonrelational (NoSQL) databases started gaining popularity. They
        aimed to improve upon the relational status quo by offering a choice of new data
        models (see Chapter 2), and by including replication (Chapter 5) and partitioning
        (Chapter 6) by default. Transactions were the main casualty of this movement: many
        of this new generation of databases abandoned transactions entirely, or redefined the
        word to describe a much weaker set of guarantees than had previously been under‐
        stood

!!!!    The truth is not that simple: like every other technical design choice, transactions
        have advantages and limitations. In order to understand those trade-offs, let’s go into
        the details of the guarantees that transactions can provide—both in normal operation
        and in various extreme (but realistic) circumstances.

        The Meaning of ACID
            The safety guarantees provided by transactions are often described by the wellknown acronym
            ACID, which stands for Atomicity, Consistency, Isolation, and Durability.

            However, in practice, one database’s implementation of ACID does not equal
            another’s implementation. For example, as we shall see, there is a lot of ambiguity
            around the meaning of isolation [8]. The high-level idea is sound, but the devil is in
            the details. Today, when a system claims to be “ACID compliant,” it’s unclear what
            guarantees you can actually expect. ACID has unfortunately become mostly a mar‐
            keting term.

            Atomicity
                In general, atomic refers to something that cannot be broken down into smaller parts.
                The word means similar but subtly different things in different branches of comput‐
                ing. For example, in multi-threaded programming, if one thread executes an atomic
                operation, that means there is no way that another thread could see the half-finished
                result of the operation. The system can only be in the state it was before the operation
                or after the operation, not something in between.

                ACID atomicity describes what happens if a client wants to make several
                writes, but a fault occurs after some of the writes have been processed—for example,
                a process crashes, a network connection is interrupted, a disk becomes full, or some
                integrity constraint is violated. If the writes are grouped together into an atomic
                transaction, and the transaction cannot be completed (committed) due to a fault, then
                the transaction is aborted and the database must discard or undo any writes it has
                made so far in that transaction.

                Perhaps abortability would have
                been a better term than atomicity, but we will stick with atomicity since that’s the
                usual word.

            Consistency
                The idea of ACID consistency is that you have certain statements about your data
                (invariants) that must always be true—for example, in an accounting system, credits
                and debits across all accounts must always be balanced. If a transaction starts with a
                database that is valid according to these invariants, and any writes during the transac‐
                tion preserve the validity, then you can be sure that the invariants are always satisfied.

                However, this idea of consistency depends on the application’s notion of invariants,
                and it’s the application’s responsibility to define its transactions correctly so that they
                preserve consistency. This is not something that the database can guarantee: if you
                write bad data that violates your invariants, the database can’t stop you

!!!!            Atomicity, isolation, and durability are properties of the database, whereas consis‐
                tency (in the ACID sense) is a property of the application. The application may rely
                on the database’s atomicity and isolation properties in order to achieve consistency,
                but it’s not up to the database alone. Thus, the letter C doesn’t really belong in ACID

            Isolation
                Most databases are accessed by several clients at the same time. That is no problem if
                they are reading and writing different parts of the database, but if they are accessing
                the same database records, you can run into concurrency problems (race conditions).
                Figure 7-1 is a simple example of this kind of problem. Say you have two clients
                simultaneously incrementing a counter that is stored in a database. Each client needs
                to read the current value, add 1, and write the new value back (assuming there is no
                increment operation built into the database). In Figure 7-1 the counter should have
                increased from 42 to 44, because two increments happened, but it actually only went
                to 43 because of the race condition.
                Isolation in the sense of ACID means that concurrently executing transactions are
                isolated from each other: they cannot step on each other’s toes. The classic database
                textbooks formalize isolation as serializability, which means that each transaction can
                pretend that it is the only transaction running on the entire database. The database
                ensures that when the transactions have committed, the result is the same as if they
                had run serially (one after another), even though in reality they may have run con‐
                currently

                However, in practice, serializable isolation is rarely used, because it carries a perfor‐
                mance penalty. Some popular databases, such as Oracle 11g, don’t even implement it.
                In Oracle there is an isolation level called “serializable,” but it actually implements
                something called snapshot isolation, which is a weaker guarantee than serializability

            Durability
                The purpose of a database system is to provide a safe place where data can be stored
                without fear of losing it. Durability is the promise that once a transaction has com‐
                mitted successfully, any data it has written will not be forgotten, even if there is a
                hardware fault or the database crashes.
                In a single-node database, durability typically means that the data has been written to
                nonvolatile storage such as a hard drive or SSD. It usually also involves a write-ahead
                log or similar (see “Making B-trees reliable” on page 82), which allows recovery in the
                event that the data structures on disk are corrupted. In a replicated database, durabil‐
                ity may mean that the data has been successfully copied to some number of nodes. In
                order to provide a durability guarantee, a database must wait until these writes or
                replications are complete before reporting a transaction as successfully committed.

        Single-Object and Multi-Object Operations
            Single-object writes
                Atomicity and isolation also apply when a single object is being changed. For exam‐
                ple, imagine you are writing a 20 KB JSON document to a database:
                    • If the network connection is interrupted after the first 10 KB have been sent, does
                    the database store that unparseable 10 KB fragment of JSON?
                    • If the power fails while the database is in the middle of overwriting the previous
                    value on disk, do you end up with the old and new values spliced together?
                    • If another client reads that document while the write is in progress, will it see a
                    partially updated value?

                Those issues would be incredibly confusing, so storage engines almost universally
                aim to provide atomicity and isolation on the level of a single object (such as a
                keyvalue pair) on one node. Atomicity can be implemented using a log for crash recov‐
                ery (see “Making B-trees reliable” on page 82), and isolation can be implemented
                using a lock on each object (allowing only one thread to access an object at any one
                time).

!!!             Some databases also provide more complex atomic operations,iv such as an increment
                operation, which removes the need for a read-modify-write cycle like that in
                Figure 7-1. Similarly popular is a compare-and-set operation, which allows a write to
                happen only if the value has not been concurrently changed by someone else (see
                “Compare-and-set” on page 245).

                These single-object operations are useful, as they can prevent lost updates when sev‐
                eral clients try to write to the same object concurrently.
                However, they are not transactions in the usual sense of the
                word. Compare-and-set and other single-object operations have been dubbed “light‐
                weight transactions” or even “ACID” for marketing purposes, but that
                terminology is misleading. A transaction is usually understood as a mechanism for
                grouping multiple operations on multiple objects into one unit of execution.

            The need for multi-object transactions
                Many distributed datastores have abandoned multi-object transactions because they
                are difficult to implement across partitions, and they can get in the way in some sce‐
                narios where very high availability or performance is required. However, there is
                nothing that fundamentally prevents transactions in a distributed database, and we
                will discuss implementations of distributed transactions in Chapter 9.

                There are some use cases in which single-object inserts, updates, and deletes are suffi‐
                cient. However, in many other cases writes to several different objects need to be
                coordinated:
                    • In a relational data model, a row in one table often has a foreign key reference to
                    a row in another table. (Similarly, in a graph-like data model, a vertex has edges
                    to other vertices.) Multi-object transactions allow you to ensure that these refer‐
                    ences remain valid: when inserting several records that refer to one another, the
                    foreign keys have to be correct and up to date, or the data becomes nonsensical.
                    • In a document data model, the fields that need to be updated together are often
                    within the same document, which is treated as a single object—no multi-object
                    transactions are needed when updating a single document. However, document
                    databases lacking join functionality also encourage denormalization (see “Rela‐
                    tional Versus Document Databases Today” on page 38). When denormalized
                    information needs to be updated, like in the example of Figure 7-2, you need to
                    update several documents in one go. Transactions are very useful in this situation
                    to prevent denormalized data from going out of sync.


    Weak Isolation Levels
        If two transactions don’t touch the same data, they can safely be run in parallel,
        because neither depends on the other. Concurrency issues (race conditions) only
        come into play when one transaction reads data that is concurrently modified by
        another transaction, or when two transactions try to simultaneously modify the same
        data.

        Concurrency bugs are hard to find by testing, because such bugs are only triggered
        when you get unlucky with the timing. Such timing issues might occur very rarely,
        and are usually difficult to reproduce. Concurrency is also very difficult to reason
        about, especially in a large application where you don’t necessarily know which other
        pieces of code are accessing the database. Application development is difficult
        enough if you just have one user at a time; having many concurrent users makes it
        much harder still, because any piece of data could unexpectedly change at any time.

!!!!    For that reason, databases have long tried to hide concurrency issues from applica‐
        tion developers by providing transaction isolation. In theory, isolation should make
        your life easier by letting you pretend that no concurrency is happening: serializable
        isolation means that the database guarantees that transactions have the same effect as
        if they ran serially (i.e., one at a time, without any concurrency).
        In practice, isolation is unfortunately not that simple. Serializable isolation has a per‐
        formance cost, and many databases don’t want to pay that price [8]. It’s therefore
        common for systems to use weaker levels of isolation, which protect against some
        concurrency issues, but not all. Those levels of isolation are much harder to under‐
        stand, and they can lead to subtle bugs, but they are nevertheless used in practice

!!!!!   Concurrency bugs caused by weak transaction isolation are not just a theoretical
        problem. They have caused substantial loss of money [24, 25], led to investigation by
        financial auditors [26], and caused customer data to be corrupted [27]. A popular
        comment on revelations of such problems is “Use an ACID database if you’re han‐
        dling financial data!”—but that misses the point. Even many popular relational data‐
        base systems (which are usually considered “ACID”) use weak isolation, so they
        wouldn’t necessarily have prevented these bugs from occurring.

!!      Rather than blindly relying on tools, we need to develop a good understanding of the
        kinds of concurrency problems that exist, and how to prevent them. Then we can
        build applications that are reliable and correct, using the tools at our disposal.

        Read Committed
            The most basic level of transaction isolation is read committed.
            It makes two guarantees:
                1. When reading from the database, you will only see data that has been committed
                (no dirty reads).
                2. When writing to the database, you will only overwrite data that has been com‐
                mitted (no dirty writes).

!!!!        No dirty reads
                Imagine a transaction has written some data to the database, but the transaction has
                not yet committed or aborted. Can another transaction see that uncommitted data? If
                yes, that is called a dirty read.
                Transactions running at the read committed isolation level must prevent dirty reads.
                This means that any writes by a transaction only become visible to others when that
                transaction commits

                There are a few reasons why it’s useful to prevent dirty reads:
                • If a transaction needs to update several objects, a dirty read means that another
                transaction may see some of the updates but not others. For example, in
                Figure 7-2, the user sees the new unread email but not the updated counter. This
                is a dirty read of the email. Seeing the database in a partially updated state is con‐
                fusing to users and may cause other transactions to take incorrect decisions.
                • If a transaction aborts, any writes it has made need to be rolled back (like in
                Figure 7-3). If the database allows dirty reads, that means a transaction may see
                data that is later rolled back—i.e., which is never actually committed to the data‐
                base. Reasoning about the consequences quickly becomes mind-bending.

!!!!        No dirty writes
                What happens if two transactions concurrently try to update the same object in a
                database? We don’t know in which order the writes will happen, but we normally
                assume that the later write overwrites the earlier write.
                However, what happens if the earlier write is part of a transaction that has not yet
                committed, so the later write overwrites an uncommitted value? This is called a dirty
                write

                By preventing dirty writes, this isolation level avoids some kinds of concurrency
                problems:
                • If transactions update multiple objects, dirty writes can lead to a bad outcome.
                For example, consider Figure 7-5, which illustrates a used car sales website on
                which two people, Alice and Bob, are simultaneously trying to buy the same car.
                Buying a car requires two database writes: the listing on the website needs to be
                updated to reflect the buyer, and the sales invoice needs to be sent to the buyer.
                In the case of Figure 7-5, the sale is awarded to Bob (because he performs the
                winning update to the listings table), but the invoice is sent to Alice (because
                she performs the winning update to the invoices table). Read committed pre‐
                vents such mishaps.
                • However, read committed does not prevent the race condition between two
                counter increments in Figure 7-1. In this case, the second write happens after the
                first transaction has committed, so it’s not a dirty write. It’s still incorrect, but for
                a different reason—in “Preventing Lost Updates” on page 242 we will discuss how
                to make such counter increments safe.

            Implementing read committed
!!!             Read committed is a very popular isolation level. It is the default setting in Oracle
                11g, PostgreSQL, SQL Server 2012, MemSQL, and many other databases [8].
                Most commonly, databases prevent dirty writes by using row-level locks: when a
                transaction wants to modify a particular object (row or document), it must first
                acquire a lock on that object. It must then hold that lock until the transaction is com‐
                mitted or aborted. Only one transaction can hold the lock for any given object; if
                another transaction wants to write to the same object, it must wait until the first
                transaction is committed or aborted before it can acquire the lock and continue. This
                locking is done automatically by databases in read committed mode (or stronger iso‐
                lation levels).

!!!             For that reason, most databases prevent dirty reads using the approach illustrated in
                Figure 7-4: for every object that is written, the database remembers both the old com‐
                mitted value and the new value set by the transaction that currently holds the write
                lock. While the transaction is ongoing, any other transactions that read the object are
                simply given the old value. Only when the new value is committed do transactions
                switch over to reading the new value.


        Snapshot Isolation and Repeatable Read
            Say Alice has $1,000 of savings at a bank, split across two accounts with $500 each.
            Now a transaction transfers $100 from one of her accounts to the other. If she is
            unlucky enough to look at her list of account balances in the same moment as that
            transaction is being processed, she may see one account balance at a time before the
            incoming payment has arrived (with a balance of $500), and the other account after
            the outgoing transfer has been made (the new balance being $400). To Alice it now
            appears as though she only has a total of $900 in her accounts—it seems that $100 has
            vanished into thin air.

            This anomaly is called a nonrepeatable read or read skew

            However, some situations cannot tolerate such temporary inconsistency:
                Backups
                    Taking a backup requires making a copy of the entire database, which may take
                    hours on a large database. During the time that the backup process is running,
                    writes will continue to be made to the database. Thus, you could end up with
                    some parts of the backup containing an older version of the data, and other parts
                    containing a newer version. If you need to restore from such a backup, the
                    inconsistencies (such as disappearing money) become permanent.
                Analytic queries and integrity checks
                    Sometimes, you may want to run a query that scans over large parts of the data‐
                    base. Such queries are common in analytics (see “Transaction Processing or Ana‐
                    lytics?” on page 90), or may be part of a periodic integrity check that everything
                    is in order (monitoring for data corruption). These queries are likely to return
                    nonsensical results if they observe parts of the database at different points in
                    time.

            Snapshot isolation [28] is the most common solution to this problem. The idea is that
            each transaction reads from a consistent snapshot of the database—that is, the trans‐
            action sees all the data that was committed in the database at the start of the transac‐
            tion. Even if the data is subsequently changed by another transaction, each
            transaction sees only the old data from that particular point in time.
            Snapshot isolation is a boon for long-running, read-only queries such as backups and
            analytics.

            Snapshot isolation is a popular feature: it is supported by PostgreSQL, MySQL with
            the InnoDB storage engine, Oracle, SQL Server, and others

            From a performance point of view, a key principle of snapshot isolation is readers
            never block writers, and writers never block readers. This allows a database to handle
            long-running read queries on a consistent snapshot at the same time as processing
            writes normally, without any lock contention between the two.

            To implement snapshot isolation, databases use a generalization of the mechanism
            we saw for preventing dirty reads in Figure 7-4. The database must potentially keep
            several different committed versions of an object, because various in-progress trans‐
            actions may need to see the state of the database at different points in time. Because it
            maintains several versions of an object side by side, this technique is known as multiversion
            concurrency control (MVCC).

!!!         Figure 7-7 illustrates how MVCC-based snapshot isolation is implemented in Post‐
            greSQL [31] (other implementations are similar). When a transaction is started, it is
            given a unique, always-increasingvii transaction ID (txid). Whenever a transaction
            writes anything to the database, the data it writes is tagged with the transaction ID of
            the writer.

!!!!        Each row in a table has a created_by field, containing the ID of the transaction that
            inserted this row into the table. Moreover, each row has a deleted_by field, which is
            initially empty. If a transaction deletes a row, the row isn’t actually deleted from the
            database, but it is marked for deletion by setting the deleted_by field to the ID of the
            transaction that requested the deletion. At some later time, when it is certain that no
            transaction can any longer access the deleted data, a garbage collection process in the
            database removes any rows marked for deletion and frees their space.

        Preventing Lost Updates
            The read committed and snapshot isolation levels we’ve discussed so far have been
            primarily about the guarantees of what a read-only transaction can see in the pres‐
            ence of concurrent writes. We have mostly ignored the issue of two transactions writ‐
            ing concurrently—we have only discussed dirty writes (see “No dirty writes” on page
            235), one particular type of write-write conflict that can occur.

            The lost update problem can occur if an application reads some value from the data‐
            base, modifies it, and writes back the modified value (a read-modify-write cycle). If
            two transactions do this concurrently, one of the modifications can be lost, because
            the second write does not include the first modification. (We sometimes say that the
            later write clobbers the earlier write.) This pattern occurs in various different
            scenarios:
                • Incrementing a counter or updating an account balance (requires reading the
                current value, calculating the new value, and writing back the updated value)
                • Making a local change to a complex value, e.g., adding an element to a list within
                a JSON document (requires parsing the document, making the change, and writ‐
                ing back the modified document)
                • Two users editing a wiki page at the same time, where each user saves their
                changes by sending the entire page contents to the server, overwriting whatever
                is currently in the database

            Atomic write operations
                Many databases provide atomic update operations, which remove the need to imple‐
                ment read-modify-write cycles in application code. They are usually the best solution
                if your code can be expressed in terms of those operations. For example, the follow‐
                ing instruction is concurrency-safe in most relational databases:
                UPDATE counters SET value = value + 1 WHERE key = 'foo';

!!!             Unfortunately, object-relational mapping frameworks make it easy to accidentally
                write code that performs unsafe read-modify-write cycles instead of using atomic
                operations provided by the database [38]. That’s not a problem if you know what you
                are doing, but it is potentially a source of subtle bugs that are difficult to find by
                testing.

            Explicit locking
                Another option for preventing lost updates, if the database’s built-in atomic opera‐
                tions don’t provide the necessary functionality, is for the application to explicitly lock
                objects that are going to be updated. Then the application can perform a readmodify-write cycle,
                and if any other transaction tries to concurrently read the same
                object, it is forced to wait until the first read-modify-write cycle has completed.

            Automatically detecting lost updates
                Atomic operations and locks are ways of preventing lost updates by forcing the readmodify-write
                cycles to happen sequentially. An alternative is to allow them to execute
                in parallel and, if the transaction manager detects a lost update, abort the transaction
                and force it to retry its read-modify-write cycle.
                An advantage of this approach is that databases can perform this check efficiently in
                conjunction with snapshot isolation. Indeed, PostgreSQL’s repeatable read, Oracle’s
                serializable, and SQL Server’s snapshot isolation levels automatically detect when a
                lost update has occurred and abort the offending transaction.

            Compare-and-set
                In databases that don’t provide transactions, you sometimes find an atomic compareand-set
                operation (previously mentioned in “Single-object writes” on page 230). The
                purpose of this operation is to avoid lost updates by allowing an update to happen
                only if the value has not changed since you last read it. If the current value does not
                match what you previously read, the update has no effect, and the read-modify-write
                cycle must be retried.
                For example, to prevent two users concurrently updating the same wiki page, you
                might try something like this, expecting the update to occur only if the content of the
                page hasn’t changed since the user started editing it:
                    -- This may or may not be safe, depending on the database implementation
                    UPDATE wiki_pages SET content = 'new content'
                     WHERE id = 1234 AND content = 'old content';


        Write Skew and Phantoms
            This effect, where a write in one transaction changes the result of a search query in
            another transaction, is called a phantom [3]. Snapshot isolation avoids phantoms in
            read-only queries, but in read-write transactions like the examples we discussed,
            phantoms can lead to particularly tricky cases of write skew

            Materializing conflicts
                If the problem of phantoms is that there is no object to which we can attach the locks,
                perhaps we can artificially introduce a lock object into the database?
                For example, in the meeting room booking case you could imagine creating a table of
                time slots and rooms. Each row in this table corresponds to a particular room for a
                particular time period (say, 15 minutes). You create rows for all possible combina‐
                tions of rooms and time periods ahead of time, e.g. for the next six months.
                Now a transaction that wants to create a booking can lock (SELECT FOR UPDATE) the
                rows in the table that correspond to the desired room and time period. After it has
                acquired the locks, it can check for overlapping bookings and insert a new booking as
                before. Note that the additional table isn’t used to store information about the book‐
                ing—it’s purely a collection of locks which is used to prevent bookings on the same
                room and time range from being modified concurrently.
                This approach is called materializing conflicts, because it takes a phantom and turns it
                into a lock conflict on a concrete set of rows that exist in the database

                Unfortunately, it can be hard and error-prone to figure out how to materialize conflicts, and
                it’s ugly to let a concurrency control mechanism leak into the application data model.
                For those reasons, materializing conflicts should be considered a last resort if no
                alternative is possible. A serializable isolation level is much preferable in most cases.

    Serializability
         It’s a sad situation:
            • Isolation levels are hard to understand, and inconsistently implemented in differ‐
            ent databases (e.g., the meaning of “repeatable read” varies significantly).
            • If you look at your application code, it’s difficult to tell whether it is safe to run at
            a particular isolation level—especially in a large application, where you might not
            be aware of all the things that may be happening concurrently.
            • There are no good tools to help us detect race conditions. In principle, static
            analysis may help, but research techniques have not yet found their way into
            practical use. Testing for concurrency issues is hard, because they are usually
            nondeterministic—problems only occur if you get unlucky with the timing.

!!!     This is not a new problem—it has been like this since the 1970s, when weak isolation
        levels were first introduced. All along, the answer from researchers has been sim‐
        ple: use serializable isolation!

!!!     Serializable isolation is usually regarded as the strongest isolation level. It guarantees
        that even though transactions may execute in parallel, the end result is the same as if
        they had executed one at a time, serially, without any concurrency. Thus, the database
        guarantees that if the transactions behave correctly when run individually, they con‐
        tinue to be correct when run concurrently—in other words, the database prevents all
        possible race conditions.

        But if serializable isolation is so much better than the mess of weak isolation levels,
        then why isn’t everyone using it? To answer this question, we need to look at the
        options for implementing serializability, and how they perform. Most databases that
        provide serializability today use one of three techniques, which we will explore in the
        rest of this chapter:
            • Literally executing transactions in a serial order
            • Two-phase locking, which for several decades was the only viable option
            • Optimistic concurrency control techniques such as serializable snapshot isolation

        Actual Serial Execution
            The simplest way of avoiding concurrency problems is to remove the concurrency
            entirely: to execute only one transaction at a time, in serial order, on a single thread.
            By doing so, we completely sidestep the problem of detecting and preventing con‐
            flicts between transactions: the resulting isolation is by definition serializable.

!!!!        The approach of executing transactions serially is implemented in VoltDB/H-Store,
            Redis, and Datomic [46, 47, 48]. A system designed for single-threaded execution can
            sometimes perform better than a system that supports concurrency, because it can
            avoid the coordination overhead of locking. However, its throughput is limited to
            that of a single CPU core.

            Encapsulating transactions in stored procedures
                In the early days of databases, the intention was that a database transaction could
                encompass an entire flow of user activity. For example, booking an airline ticket is a
                multi-stage process (searching for routes, fares, and available seats; deciding on an
                itinerary; booking seats on each of the flights of the itinerary; entering passenger
                details; making payment). Database designers thought that it would be neat if that
                entire process was one transaction so that it could be committed atomically.

                Unfortunately, humans are very slow to make up their minds and respond. If a data‐
                base transaction needs to wait for input from a user, the database needs to support a
                potentially huge number of concurrent transactions, most of them idle. Most data‐
                bases cannot do that efficiently, and so almost all OLTP applications keep transac‐
                tions short by avoiding interactively waiting for a user within a transaction. On the
                web, this means that a transaction is committed within the same HTTP request—a
                transaction does not span multiple requests. A new HTTP request starts a new trans‐
                action.

!!!!!           For this reason, systems with single-threaded serial transaction processing don’t
                allow interactive multi-statement transactions. Instead, the application must submit
                the entire transaction code to the database ahead of time, as a stored procedure. The
                differences between these approaches is illustrated in Figure 7-9. Provided that all
                data required by a transaction is in memory, the stored procedure can execute very
                fast, without waiting for any network or disk I/O.

                Dan: see "7-9 interactive transactions vs stored procedure .png"

                Pros and cons of stored procedures
                Stored procedures have existed for some time in relational databases, and they have
                been part of the SQL standard (SQL/PSM) since 1999. They have gained a somewhat
                bad reputation, for various reasons:
                    • Each database vendor has its own language for stored procedures (Oracle has PL/
                    SQL, SQL Server has T-SQL, PostgreSQL has PL/pgSQL, etc.). These languages
                    haven’t kept up with developments in general-purpose programming languages,
                    so they look quite ugly and archaic from today’s point of view, and they lack the
                    ecosystem of libraries that you find with most programming languages.
                    • Code running in a database is difficult to manage: compared to an application
                    server, it’s harder to debug, more awkward to keep in version control and deploy,
                    trickier to test, and difficult to integrate with a metrics collection system for
                    monitoring.
                    • A database is often much more performance-sensitive than an application server,
                    because a single database instance is often shared by many application servers. A
                    badly written stored procedure (e.g., using a lot of memory or CPU time) in a
                    database can cause much more trouble than equivalent badly written code in an
                    application server.
                However, those issues can be overcome. Modern implementations of stored proce‐
                dures have abandoned PL/SQL and use existing general-purpose programming lan‐
                guages instead: VoltDB uses Java or Groovy, Datomic uses Java or Clojure, and Redis
                uses Lua.

        Two-Phase Locking (2PL)
            For around 30 years, there was only one widely used algorithm for serializability in
            databases: two-phase locking
            Note that while two-phase locking (2PL) sounds very similar to
            two-phase commit (2PC), they are completely different things.

            Two-phase locking is similar, but makes the lock requirements much stronger. Sev‐
            eral transactions are allowed to concurrently read the same object as long as nobody
            is writing to it. But as soon as anyone wants to write (modify or delete) an object,
            exclusive access is required:
                • If transaction A has read an object and transaction B wants to write to that
                object, B must wait until A commits or aborts before it can continue. (This
                ensures that B can’t change the object unexpectedly behind A’s back.)
                • If transaction A has written an object and transaction B wants to read that object,
                B must wait until A commits or aborts before it can continue. (Reading an old
                version of the object, like in Figure 7-1, is not acceptable under 2PL.)

!!!         In 2PL, writers don’t just block other writers; they also block readers and vice versa.
            because 2PL provides serializability, it protects against all the race conditions dis‐
            cussed earlier, including lost updates and write skew.

!!!         The blocking of readers and writers is implemented by a having a lock on each object
            in the database. The lock can either be in shared mode or in exclusive mode. The lock
            is used as follows:
                • If a transaction wants to read an object, it must first acquire the lock in shared
                mode. Several transactions are allowed to hold the lock in shared mode simulta‐
                neously, but if another transaction already has an exclusive lock on the object,
                these transactions must wait.
                • If a transaction wants to write to an object, it must first acquire the lock in exclu‐
                sive mode. No other transaction may hold the lock at the same time (either in
                shared or in exclusive mode), so if there is any existing lock on the object, the
                transaction must wait.
                • If a transaction first reads and then writes an object, it may upgrade its shared
                lock to an exclusive lock. The upgrade works the same as getting an exclusive
                lock directly.
                • After a transaction has acquired the lock, it must continue to hold the lock until
                the end of the transaction (commit or abort). This is where the name “twophase” comes from: the first phase (while the transaction is executing) is when
                the locks are acquired, and the second phase (at the end of the transaction) is
                when all the locks are released.
            Since so many locks are in use, it can happen quite easily that transaction A is stuck
            waiting for transaction B to release its lock, and vice versa. This situation is called
            deadlock. The database automatically detects deadlocks between transactions and
            aborts one of them so that the others can make progress. The aborted transaction
            needs to be retried by the application

!!!!        The big downside of two-phase locking, and the reason why it hasn’t been used by
            everybody since the 1970s, is performance: transaction throughput and response
            times of queries are significantly worse under two-phase locking than under weak
            isolation.
            This is partly due to the overhead of acquiring and releasing all those locks, but more
            importantly due to reduced concurrency. By design, if two concurrent transactions
            try to do anything that may in any way result in a race condition, one has to wait for
            the other to complete

            Although deadlocks can happen with the lock-based read committed isolation level,
            they occur much more frequently under 2PL serializable isolation (depending on the
            access patterns of your transaction). This can be an additional performance problem:
            when a transaction is aborted due to deadlock and is retried, it needs to do its work
            all over again. If deadlocks are frequent, this can mean significant wasted effort.

        Serializable Snapshot Isolation (SSI)
            This chapter has painted a bleak picture of concurrency control in databases. On the
            one hand, we have implementations of serializability that don’t perform well (twophase locking)
            or don’t scale well (serial execution). On the other hand, we have weak
            isolation levels that have good performance, but are prone to various race conditions
            (lost updates, write skew, phantoms, etc.). Are serializable isolation and good perfor‐
            mance fundamentally at odds with each other?

!!!!        Perhaps not: an algorithm called serializable snapshot isolation (SSI) is very promis‐
            ing. It provides full serializability, but has only a small performance penalty com‐
            pared to snapshot isolation. SSI is fairly new: it was first described in 2008 and is
            the subject of Michael Cahill’s PhD thesis.
!!!         Today SSI is used both in single-node databases (the serializable isolation level in
            PostgreSQL since version 9.1) and distributed databases (FoundationDB uses a
            similar algorithm). As SSI is so young compared to other concurrency control mech‐
            anisms, it is still proving its performance in practice, but it has the possibility of being
            fast enough to become the new default in the future.


========================================================================================================================
Chapter 8. The Trouble with Distributed Systems

    However, even though we have talked a lot about faults, the last few chapters have
    still been too optimistic. The reality is even darker. We will now turn our pessimism
    to the maximum and assume that anything that can go wrong will go wrong

!!! Working with distributed systems is fundamentally different from writing software
    on a single computer—and the main difference is that there are lots of new and excit‐
    ing ways for things to go wrong [1, 2]. In this chapter, we will get a taste of the prob‐
    lems that arise in practice, and an understanding of the things we can and cannot rely
    on.

    In a distributed system, there may well be some parts of the system that are broken in
    some unpredictable way, even though other parts of the system are working fine. This
    is known as a partial failure. The difficulty is that partial failures are nondeterministic:
    if you try to do anything involving multiple nodes and the network, it may sometimes
    work and sometimes unpredictably fail. As we shall see, you may not even know
    whether something succeeded or not, as the time it takes for a message to travel
    across a network is also nondeterministic!

     a supercomputer is more like a single-node
    computer than a distributed system: it deals with partial failure by letting it escalate
    into total failure—if any part of the system fails, just let everything crash (like a kernel
    panic on a single machine)

    In this book we focus on systems for implementing internet services, which usually
    look very different from supercomputers:
        • Many internet-related applications are online, in the sense that they need to be
        able to serve users with low latency at any time. Making the service unavailable—
        for example, stopping the cluster for repair—is not acceptable. In contrast, off‐
        line (batch) jobs like weather simulations can be stopped and restarted with fairly
        low impact.
        • Supercomputers are typically built from specialized hardware, where each node
        is quite reliable, and nodes communicate through shared memory and remote
        direct memory access (RDMA). On the other hand, nodes in cloud services are
        built from commodity machines, which can provide equivalent performance at
        lower cost due to economies of scale, but also have higher failure rates.
        • Large datacenter networks are often based on IP and Ethernet, arranged in Clos
        topologies to provide high bisection bandwidth [9]. Supercomputers often use
        specialized network topologies, such as multi-dimensional meshes and toruses
        [10], which yield better performance for HPC workloads with known communi‐
        cation patterns.
        • The bigger a system gets, the more likely it is that one of its components is bro‐
        ken. Over time, broken things get fixed and new things break, but in a system
        with thousands of nodes, it is reasonable to assume that something is always bro‐
        ken [7]. When the error handling strategy consists of simply giving up, a large
        system can end up spending a lot of its time recovering from faults rather than
        doing useful work [8].
        • If the system can tolerate failed nodes and still keep working as a whole, that is a
        very useful feature for operations and maintenance: for example, you can per‐
        form a rolling upgrade (see Chapter 4), restarting one node at a time, while the
        service continues serving users without interruption. In cloud environments, if
        one virtual machine is not performing well, you can just kill it and request a new
        one (hoping that the new one will be faster).
        • In a geographically distributed deployment (keeping data geographically close to
        your users to reduce access latency), communication most likely goes over the
        internet, which is slow and unreliable compared to local networks. Supercom‐
        puters generally assume that all of their nodes are close together.

!!! If we want to make distributed systems work, we must accept the possibility of partial
    failure and build fault-tolerance mechanisms into the software. In other words, we
    need to build a reliable system from unreliable components

!!! It would be unwise to assume that faults are rare and simply hope for the best. It is
    important to consider a wide range of possible faults—even fairly unlikely ones—and
    to artificially create such situations in your testing environment to see what happens.
    In distributed systems, suspicion, pessimism, and paranoia pay off.

    Building a Reliable System from Unreliable Components
        • IP (the Internet Protocol) is unreliable: it may drop, delay, duplicate, or reorder
        packets. TCP (the Transmission Control Protocol) provides a more reliable
        transport layer on top of IP: it ensures that missing packets are retransmitted,
        duplicates are eliminated, and packets are reassembled into the order in which
        they were sent.


    Unreliable Networks
        shared-nothing systems: i.e., a bunch of machines connected by a network.

        Shared-nothing is not the only way of building systems, but it has become the domi‐
        nant approach for building internet services, for several reasons: it’s comparatively
        cheap because it requires no special hardware, it can make use of commoditized
        cloud computing services, and it can achieve high reliability through redundancy
        across multiple geographically distributed datacenters

        Detecting Faults
            Many systems need to automatically detect faulty nodes. For example:
            • A load balancer needs to stop sending requests to a node that is dead (i.e., take it
            out of rotation).
            • In a distributed database with single-leader replication, if the leader fails, one of
            the followers needs to be promoted to be the new leader

        Timeouts and Unbounded Delays
            If a timeout is the only sure way of detecting a fault, then how long should the time‐
            out be? There is unfortunately no simple answer.
            A long timeout means a long wait until a node is declared dead (and during this time,
            users may have to wait or see error messages). A short timeout detects faults faster,
            but carries a higher risk of incorrectly declaring a node dead when in fact it has only
            suffered a temporary slowdown (e.g., due to a load spike on the node or the network).

            TCP performs flow control (also known as congestion avoidance or backpressure),
            in which a node limits its own rate of sending in order to avoid overloading a
            network link or the receiving node [27]. This means additional queueing at the
            sender before the data even enters the network.
            Moreover, TCP considers a packet to be lost if it is not acknowledged within some
            timeout (which is calculated from observed round-trip times), and lost packets are
            automatically retransmitted. Although the application does not see the packet loss
            and retransmission, it does see the resulting delay (waiting for the timeout to expire,
            and then waiting for the retransmitted packet to be acknowledged).

            TCP Versus UDP
                Some latency-sensitive applications, such as videoconferencing and Voice over IP
                (VoIP), use UDP rather than TCP. It’s a trade-off between reliability and variability
                of delays: as UDP does not perform flow control and does not retransmit lost packets,
                it avoids some of the reasons for variable network delays (although it is still suscepti‐
                ble to switch queues and scheduling delays).
                UDP is a good choice in situations where delayed data is worthless. For example, in a
                VoIP phone call, there probably isn’t enough time to retransmit a lost packet before
                its data is due to be played over the loudspeakers. In this case, there’s no point in
                retransmitting the packet—the application must instead fill the missing packet’s time
                slot with silence (causing a brief interruption in the sound) and move on in the
                stream. The retry happens at the human layer instead. (“Could you repeat that please?
                The sound just cut out for a moment.”)

    Unreliable Clocks
        Clocks and time are important. Applications depend on clocks in various ways to
        answer questions like the following:
            1. Has this request timed out yet?
            2. What’s the 99th percentile response time of this service?
            3. How many queries per second did this service handle on average in the last five
            minutes?
            4. How long did the user spend on our site?
            5. When was this article published?
            6. At what date and time should the reminder email be sent?
            7. When does this cache entry expire?
            8. What is the timestamp on this error message in the log file?
        Examples 1–4 measure durations (e.g., the time interval between a request being sent
        and a response being received), whereas examples 5–8 describe points in time (events
        that occur on a particular date, at a particular time).

        In a distributed system, time is a tricky business, because communication is not
        instantaneous: it takes time for a message to travel across the network from one
        machine to another. The time when a message is received is always later than the
        time when it is sent, but due to variable delays in the network, we don’t know how
        much later. This fact sometimes makes it difficult to determine the order in which
        things happened when multiple machines are involved

        Monotonic Versus Time-of-Day Clocks
            Modern computers have at least two different kinds of clocks: a time-of-day clock and
            a monotonic clock. Although they both measure time, it is important to distinguish
            the two, since they serve different purposes.
                Time-of-day clocks
                    A time-of-day clock does what you intuitively expect of a clock: it returns the current
                    date and time according to some calendar (also known as wall-clock time). For exam‐
!!!                 ple, clock_gettime(CLOCK_REALTIME) on Linux and System.currentTimeMillis()
                    in Java return the number of seconds (or milliseconds) since the epoch: midnight
                    UTC on January 1, 1970, according to the Gregorian calendar, not counting leap sec‐
                    onds. Some systems use other dates as their reference point.
                    Time-of-day clocks are usually synchronized with NTP, which means that a time‐
                    stamp from one machine (ideally) means the same as a timestamp on another
                    machine. However, time-of-day clocks also have various oddities, as described in the
                    next section. In particular, if the local clock is too far ahead of the NTP server, it may
                    be forcibly reset and appear to jump back to a previous point in time.

                Monotonic clocks
!!!                 A monotonic clock is suitable for measuring a duration (time interval), such as a
                    timeout or a service’s response time: clock_gettime(CLOCK_MONOTONIC) on Linux
                    and System.nanoTime() in Java are monotonic clocks, for example. The name comes
                    from the fact that they are guaranteed to always move forward (whereas a time-of day
                    clock may jump back in time)

                    You can check the value of the monotonic clock at one point in time, do something,
                    and then check the clock again at a later time. The difference between the two values
                    tells you how much time elapsed between the two checks. However, the absolute
                    value of the clock is meaningless: it might be the number of nanoseconds since the
                    computer was started, or something similarly arbitrary. In particular, it makes no
                    sense to compare monotonic clock values from two different computers, because they
                    don’t mean the same thing.

        It is possible to achieve very good clock accuracy if you care about it sufficiently to
        invest significant resources. For example, the MiFID II draft European regulation for
        financial institutions requires all high-frequency trading funds to synchronize their
        clocks to within 100 microseconds of UTC, in order to help debug market anomalies
        such as “flash crashes” and to help detect market manipulation

        The same is true with clocks: although they
        work quite well most of the time, robust software needs to be prepared to deal with
        incorrect clocks.

        Timestamps for ordering events
            Let’s consider one particular situation in which it is tempting, but dangerous, to rely
            on clocks: ordering of events across multiple nodes. For example, if two clients write
            to a distributed database, who got there first? Which write is the more recent one?

            So-called logical clocks [56, 57], which are based on incrementing counters rather
            than an oscillating quartz crystal, are a safer alternative for ordering events (see
            “Detecting Concurrent Writes” on page 184). Logical clocks do not measure the time
            of day or the number of seconds elapsed, only the relative ordering of events
            (whether one event happened before or after another). In contrast, time-of-day and
            monotonic clocks, which measure actual elapsed time, are also known as physical
            clocks. We’ll look at ordering a bit more in “Ordering Guarantees” on page 339.

!!!         On a single-node database, a simple counter is sufficient for generating
            transaction IDs.
            However, when a database is distributed across many machines, potentially in multi‐
            ple datacenters, a global, monotonically increasing transaction ID (across all parti‐
            tions) is difficult to generate, because it requires coordination. The transaction ID
            must reflect causality: if transaction B reads a value that was written by transaction A,
            then B must have a higher transaction ID than A—otherwise, the snapshot would not
            be consistent. With lots of small, rapid transactions, creating transaction IDs in a dis‐
            tributed system becomes an untenable bottleneck.

            Can we use the timestamps from synchronized time-of-day clocks as transaction IDs?
            If we could get the synchronization good enough, they would have the right proper‐
            ties: later transactions have a higher timestamp. The problem, of course, is the uncer‐
            tainty about clock accuracy.

            Google deploys a GPS receiver or atomic clock in each
            datacenter, allowing clocks to be synchronized to within about 7 ms

        Process pauses
            When writing multi-threaded code on a single machine, we have fairly good tools for
            making it thread-safe: mutexes, semaphores, atomic counters, lock-free data struc‐
            tures, blocking queues, and so on. Unfortunately, these tools don’t directly translate
            to distributed systems, because a distributed system has no shared memory—only
            messages sent over an unreliable network.
            A node in a distributed system must assume that its execution can be paused for a
            significant length of time at any point, even in the middle of a function. During the
            pause, the rest of the world keeps moving and may even declare the paused node
            dead because it’s not responding. Eventually, the paused node may continue running,
            without even noticing that it was asleep until it checks its clock sometime later.


    Knowledge, Truth, and Lies
        The Truth Is Defined by the Majority
            n. A distributed system cannot exclusively rely on a single node, because a
            node may fail at any time, potentially leaving the system stuck and unable to recover.
            Instead, many distributed algorithms rely on a quorum, that is, voting among the
            nodes (see “Quorums for reading and writing” on page 179): decisions require some
            minimum number of votes from several nodes in order to reduce the dependence on
            any one particular node.
            That includes decisions about declaring nodes dead. If a quorum of nodes declares
            another node dead, then it must be considered dead, even if that node still very much
            feels alive. The individual node must abide by the quorum decision and step down.


        Bizantine faults
            A system is Byzantine fault-tolerant if it continues to operate correctly even if some
            of the nodes are malfunctioning and not obeying the protocol, or if malicious attack‐
            ers are interfering with the network. This concern is relevant in certain specific cir‐
            cumstances. For example:
                • In aerospace environments, the data in a computer’s memory or CPU register
                could become corrupted by radiation, leading it to respond to other nodes in
                arbitrarily unpredictable ways. Since a system failure would be very expensive
                (e.g., an aircraft crashing and killing everyone on board, or a rocket colliding
                with the International Space Station), flight control systems must tolerate Byzan‐
                tine faults [81, 82].
                • In a system with multiple participating organizations, some participants may
                attempt to cheat or defraud others. In such circumstances, it is not safe for a
                node to simply trust another node’s messages, since they may be sent with mali‐
                cious intent. For example, peer-to-peer networks like Bitcoin and other block‐
                chains can be considered to be a way of getting mutually untrusting parties to
                agree whether a transaction happened or not, without relying on a central
                authority [83].



========================================================================================================================
Chapter 9. Consistency and Consensus

    Lots of things can go wrong in distributed systems, as discussed in Chapter 8. The
    simplest way of handling such faults is to simply let the entire service fail, and show
    the user an error message. If that solution is unacceptable, we need to find ways of
    tolerating faults—that is, of keeping the service functioning correctly, even if some
    internal component is faulty.
    In this chapter, we will talk about some examples of algorithms and protocols for
    building fault-tolerant distributed systems. We will assume that all the problems
    from Chapter 8 can occur: packets can be lost, reordered, duplicated, or arbitrarily
    delayed in the network; clocks are approximate at best; and nodes can pause (e.g., due
    to garbage collection) or crash at any time.

!!!!one of the most important abstractions for distributed systems is consensus: that is,
    getting all of the nodes to agree on something

    Consistency Guarantees
        In “Problems with Replication Lag” on page 161 we looked at some timing issues that
        occur in a replicated database. If you look at two database nodes at the same moment
        in time, you’re likely to see different data on the two nodes, because write requests
        arrive on different nodes at different times. These inconsistencies occur no matter
        what replication method the database uses (single-leader, multi-leader, or leaderless
        replication).

        Most replicated databases provide at least eventual consistency, which means that if
        you stop writing to the database and wait for some unspecified length of time, then
        eventually all read requests will return the same value

        When working with a database that provides only weak guarantees, you need to be
        constantly aware of its limitations and not accidentally assume too much. Bugs are
        often subtle and hard to find by testing, because the application may work well most
        of the time. The edge cases of eventual consistency only become apparent when there
        is a fault in the system (e.g., a network interruption) or at high concurrency.

        In this chapter we will explore stronger consistency models that data systems may
        choose to provide. They don’t come for free: systems with stronger guarantees may
        have worse performance or be less fault-tolerant than systems with weaker guaran‐
        tees. Nevertheless, stronger guarantees can be appealing because they are easier to use
        correctly. Once you have seen a few different consistency models, you’ll be in a better
        position to decide which one best fits your needs.

    Linearizability
        In an eventually consistent database, if you ask two different replicas the same ques‐
        tion at the same time, you may get two different answers. That’s confusing. Wouldn’t
        it be a lot simpler if the database could give the illusion that there is only one replica
        (i.e., only one copy of the data)? Then every client would have the same view of the
        data, and you wouldn’t have to worry about replication lag.
        This is the idea behind linearizability [6] (also known as atomic consistency [7], strong
        consistency, immediate consistency, or external consistency [8]). The exact definition
        of linearizability is quite subtle, and we will explore it in the rest of this section. But
!!!!    the basic idea is to make a system appear as if there were only one copy of the data,
        and all operations on it are atomic. With this guarantee, even though there may be
        multiple replicas in reality, the application does not need to worry about them.

        In a linearizable system, as soon as one client successfully completes a write, all cli‐
        ents reading from the database must be able to see the value just written. Maintaining
        the illusion of a single copy of the data means guaranteeing that the value read is the
        most recent, up-to-date value, and doesn’t come from a stale cache or replica. In
        other words, linearizability is a recency guarantee.

        In a linearizable system we imagine that there must be some point in time (between
        the start and end of the write operation) at which the value of x atomically flips from
        0 to 1. Thus, if one client’s read returns the new value 1, all subsequent reads must
        also return the new value, even if the write operation has not yet completed.

        Relying on Linearizability
            A system that uses single-leader replication needs to ensure that there is indeed only
            one leader, not several (split brain). One way of electing a leader is to use a lock: every
            node that starts up tries to acquire the lock, and the one that succeeds becomes the
            leader [14]. No matter how this lock is implemented, it must be linearizable: all nodes
            must agree which node owns the lock; otherwise it is useless.
            Coordination services like Apache ZooKeeper [15] and etcd [16] are often used to
            implement distributed locks and leader election

!!!         Uniqueness constraints are common in databases: for example, a username or email
            address must uniquely identify one user, and in a file storage service there cannot be
            two files with the same path and filename. If you want to enforce this constraint as
            the data is written (such that if two people try to concurrently create a user or a file
            with the same name, one of them will be returned an error), you need linearizability

            This situation is actually similar to a lock: when a user registers for your service, you
            can think of them acquiring a “lock” on their chosen username. The operation is also
            very similar to an atomic compare-and-set, setting the username to the ID of the user
            who claimed it, provided that the username is not already taken.

            Similar issues arise if you want to ensure that a bank account balance never goes neg‐
            ative, or that you don’t sell more items than you have in stock in the warehouse, or
            that two people don’t concurrently book the same seat on a flight or in a theater.
            These constraints all require there to be a single up-to-date value (the account bal‐
            ance, the stock level, the seat occupancy) that all nodes agree on.

        The Cost of Linearizability
            As some replication methods can provide linearizability and others cannot, it is inter‐
            esting to explore the pros and cons of linearizability in more depth.

            Can’t we maybe find a more efficient implementation of linearizable storage? It
            seems the answer is no: Attiya and Welch [47] prove that if you want linearizability,
            the response time of read and write requests is at least proportional to the uncertainty
            of delays in the network. In a network with highly variable delays, like most com‐
            puter networks (see “Timeouts and Unbounded Delays” on page 281), the response
            time of linearizable reads and writes is inevitably going to be high. A faster algorithm
            for linearizability does not exist, but weaker consistency models can be much faster,
            so this trade-off is important for latency-sensitive systems

    Ordering Guarantees
        We said previously that a linearizable register behaves as if there is only a single copy
        of the data, and that every operation appears to take effect atomically at one point in
        time. This definition implies that operations are executed in some well-defined order.
        We illustrated the ordering in Figure 9-4 by joining up the operations in the order in
        which they seem to have executed.
        Ordering has been a recurring theme in this book, which suggests that it might be an
        important fundamental idea.

        Ordering and Causality
            There are several reasons why ordering keeps coming up, and one of the reasons is
            that it helps preserve causality.

            If a system obeys the ordering imposed by causality, we say that it is causally consis‐
            tent. For example, snapshot isolation provides causal consistency: when you read
            from the database, and you see some piece of data, then you must also be able to see
            any data that causally precedes it (assuming it has not been deleted in the meantime).

            So what is the relationship between the causal order and linearizability? The answer is
            that linearizability implies causality: any system that is linearizable will preserve cau‐
            sality correctly [7]. In particular, if there are multiple communication channels in a
            system (such as the message queue and the file storage service in Figure 9-5), lineariz‐
            ability ensures that causality is automatically preserved without the system having to
            do anything special (such as passing around timestamps between different compo‐
            nents).

       Dan: i went fast over this section...way too many details for me right now. This is best to read when needed

    Distributed Transactions and Consensus
        Consensus is one of the most important and fundamental problems in distributed
        computing. On the surface, it seems simple: informally, the goal is simply to get sev‐
        eral nodes to agree on something. You might think that this shouldn’t be too hard.
        Unfortunately, many broken systems have been built in the mistaken belief that this
        problem is easy to solve.
        Although consensus is very important, the section about it appears late in this book
        because the topic is quite subtle, and appreciating the subtleties requires some pre‐
        requisite knowledge. Even in the academic research community, the understanding
        of consensus only gradually crystallized over the course of decades, with many mis‐
        understandings along the way

!!!!    There are a number of situations in which it is important for nodes to agree. For
        example:
            Leader election
                In a database with single-leader replication, all nodes need to agree on which
                node is the leader. The leadership position might become contested if some
                nodes can’t communicate with others due to a network fault
            Atomic commit
                In a database that supports transactions spanning several nodes or partitions, we
                have the problem that a transaction may fail on some nodes but succeed on oth‐
                ers. If we want to maintain transaction atomicity (in the sense of ACID; see
                “Atomicity” on page 223), we have to get all nodes to agree on the outcome of the
                transaction: either they all abort/roll back (if anything goes wrong) or they all
                commit (if nothing goes wrong). This instance of consensus is known as the
                atomic commit problem

        Atomic Commit and Two-Phase Commit (2PC)
            In Chapter 7 we learned that the purpose of transaction atomicity is to provide sim‐
            ple semantics in the case where something goes wrong in the middle of making sev‐
            eral writes. The outcome of a transaction is either a successful commit, in which case
            all of the transaction’s writes are made durable, or an abort, in which case all of the
            transaction’s writes are rolled back (i.e., undone or discarded).

            However, what if multiple nodes are involved in a transaction? For example, perhaps
            you have a multi-object transaction in a partitioned database, or a term-partitioned
            secondary index (in which the index entry may be on a different node from the pri‐
            mary data.
            In these cases, it is not sufficient to simply send a commit request to all of the nodes
            and independently commit the transaction on each one

            If some nodes commit the transaction but others abort it, the nodes become inconsis‐
            tent with each other (like in Figure 7-3). And once a transaction has been committed
            on one node, it cannot be retracted again if it later turns out that it was aborted on
            another node. For this reason, a node must only commit once it is certain that all
            other nodes in the transaction are also going to commit

            A transaction commit must be irrevocable—you are not allowed to change your
            mind and retroactively abort a transaction after it has been committed. The reason
            for this rule is that once data has been committed, it becomes visible to other transac‐
            tions, and thus other clients may start relying on that data;

!!!!!       Two-phase commit is an algorithm for achieving atomic transaction commit across
            multiple nodes—i.e., to ensure that either all nodes commit or all nodes abort. It is a
            classic algorithm in distributed databases [13, 35, 75]. 2PC is used internally in some
            databases and also made available to applications in the form of XA transactions [76,
            77] (which are supported by the Java Transaction API, for example) or via WSAtomicTransaction for SOAP web services [78, 79].
            The basic flow of 2PC is illustrated in Figure 9-9. Instead of a single commit request,
            as with a single-node transaction, the commit/abort process in 2PC is split into two
            phases (hence the name).

            Dan see "9-9 two phase commit.png"

!!!         2PC uses a new component that does not normally appear in single-node transac‐
            tions: a coordinator (also known as transaction manager).
            The coordinator is often
            implemented as a library within the same application process that is requesting the
            transaction (e.g., embedded in a Java EE container), but it can also be a separate pro‐
            cess or service.

!!!!!       When the application is ready to commit, the coordinator begins phase 1: it
            sends a prepare request to each of the nodes, asking them whether they are able to
            commit. The coordinator then tracks the responses from the participants:
                • If all participants reply “yes,” indicating they are ready to commit, then the coor‐
                dinator sends out a commit request in phase 2, and the commit actually takes
                place.
                • If any of the participants replies “no,” the coordinator sends an abort request to
                all nodes in phase 2.
!!!         This process is somewhat like the traditional marriage ceremony in Western cultures:
            the minister asks the bride and groom individually whether each wants to marry the
            other, and typically receives the answer “I do” from both. After receiving both
            acknowledgments, the minister pronounces the couple husband and wife: the trans‐
            action is committed, and the happy fact is broadcast to all attendees. If either bride or
            groom does not say “yes,” the ceremony is aborted

            If the coordinator fails before sending the prepare requests, a participant can safely
            abort the transaction. But once the participant has received a prepare request and
            voted “yes,” it can no longer abort unilaterally—it must wait to hear back from the
            coordinator whether the transaction was committed or aborted. If the coordinator
            crashes or the network fails at this point, the participant can do nothing but wait. A
            participant’s transaction in this state is called in doubt or uncertain

        Distributed Transactions in Practice
!!!!        Distributed transactions, especially those implemented with two-phase commit, have
            a mixed reputation. On the one hand, they are seen as providing an important safety
            guarantee that would be hard to achieve otherwise; on the other hand, they are criti‐
            cized for causing operational problems, killing performance, and promising more
            than they can deliver [81, 82, 83, 84]. Many cloud services choose not to implement
            distributed transactions due to the operational problems they engender

            Some implementations of distributed transactions carry a heavy performance penalty
            —for example, distributed transactions in MySQL are reported to be over 10 times
            slower than single-node transactions [87], so it is not surprising when people advise
            against using them.

            However, rather than dismissing distributed transactions outright, we should exam‐
            ine them in some more detail, because there are important lessons to be learned from
            them. To begin, we should be precise about what we mean by “distributed transac‐
            tions.” Two quite different types of distributed transactions are often conflated:
                Database-internal distributed transactions
                    Some distributed databases (i.e., databases that use replication and partitioning
                    in their standard configuration) support internal transactions among the nodes
                    of that database. For example, VoltDB and MySQL Cluster’s NDB storage engine
                    have such internal transaction support. In this case, all the nodes participating in
                    the transaction are running the same database software.
                Heterogeneous distributed transactions
                    In a heterogeneous transaction, the participants are two or more different tech‐
                    nologies: for example, two databases from different vendors, or even non database systems such
                    as message brokers. A distributed transaction across these
                    systems must ensure atomic commit, even though the systems may be entirely
                    different under the hood.


            XA transactions
                X/Open XA (short for eXtended Architecture) is a standard for implementing two phase
                commit across heterogeneous technologies. It was introduced in 1991
                and has been widely implemented: XA is supported by many traditional relational
                databases (including PostgreSQL, MySQL, DB2, SQL Server, and Oracle) and mes‐
                sage brokers (including ActiveMQ, HornetQ, MSMQ, and IBM MQ)

                XA is not a network protocol—it is merely a C API for interfacing with a transaction
                coordinator. Bindings for this API exist in other languages; for example, in the world
                of Java EE applications, XA transactions are implemented using the Java Transaction
                API (JTA), which in turn is supported by many drivers for databases using Java Data‐
                base Connectivity (JDBC) and drivers for message brokers using the Java Message
                Service (JMS) APIs.






========================================================================================================================
========================================================================================================================
========================================================================================================================
========================================================================================================================
PART 3 Derived Data

    In this final part of the book, we will examine the issues around integrating multiple
    different data systems, potentially with different data models and optimized for dif‐
    ferent access patterns, into one coherent application architecture. This aspect of
    system-building is often overlooked by vendors who claim that their product can sat‐
    isfy all your needs. In reality, integrating disparate systems is one of the most impor‐
    tant things that needs to be done in a nontrivial application.

!!!!!
    On a high level, systems that store and process data can be grouped into two broad
    categories:
        Systems of record
            A system of record, also known as source of truth, holds the authoritative version
            of your data. When new data comes in, e.g., as user input, it is first written here.
            Each fact is represented exactly once (the representation is typically normalized).
            If there is any discrepancy between another system and the system of record,
            then the value in the system of record is (by definition) the correct one.
        Derived data systems
            Data in a derived system is the result of taking some existing data from another
            system and transforming or processing it in some way. If you lose derived data,
            you can recreate it from the original source. A classic example is a cache: data can
            be served from the cache if present, but if the cache doesn’t contain what you
            need, you can fall back to the underlying database. Denormalized values, indexes,
            and materialized views also fall into this category. In recommendation systems,
            predictive summary data is often derived from usage logs.

    Not all systems make a clear distinction between systems of record and derived data
    in their architecture, but it’s a very helpful distinction to make, because it clarifies the
    dataflow through your system: it makes explicit which parts of the system have which
    inputs and which outputs, and how they depend on each other

    By being clear about which data is derived from which other data, you can bring
    clarity to an otherwise confusing system architecture. This point will be a running
    theme throughout this part of the book.


========================================================================================================================
Chapter 10. Batch Processing

!!!!
    The web, and increasing numbers of HTTP/REST-based APIs, has made the request/
    response style of interaction so common that it’s easy to take it for granted. But we
    should remember that it’s not the only way of building systems, and that other
    approaches have their merits too. Let’s distinguish three different types of systems:
        Services (online systems)
            A service waits for a request or instruction from a client to arrive. When one is
            received, the service tries to handle it as quickly as possible and sends a response
            back. Response time is usually the primary measure of performance of a service,
            and availability is often very important (if the client can’t reach the service, the
            user will probably get an error message).
        Batch processing systems (offline systems)
            A batch processing system takes a large amount of input data, runs a job to pro‐
            cess it, and produces some output data. Jobs often take a while (from a few
            minutes to several days), so there normally isn’t a user waiting for the job to fin‐
            ish. Instead, batch jobs are often scheduled to run periodically (for example, once
            a day). The primary performance measure of a batch job is usually throughput
            (the time it takes to crunch through an input dataset of a certain size). We dis‐
            cuss batch processing in this chapter.
        Stream processing systems (near-real-time systems)
            Stream processing is somewhere between online and offline/batch processing (so
            it is sometimes called near-real-time or nearline processing). Like a batch pro‐
            cessing system, a stream processor consumes inputs and produces outputs
            (rather than responding to requests). However, a stream job operates on events
            shortly after they happen, whereas a batch job operates on a fixed set of input
            data. This difference allows stream processing systems to have lower latency than
            the equivalent batch systems. As stream processing builds upon batch process‐
            ing, we discuss it in Chapter 11.

    As we shall see in this chapter, batch processing is an important building block in our
    quest to build reliable, scalable, and maintainable applications. For example, Map‐
    Reduce, a batch processing algorithm published in 2004 [1], was (perhaps over enthusiastically)
    called “the algorithm that makes Google so massively scalable” [2]. It
    was subsequently implemented in various open source data systems, including
    Hadoop, CouchDB, and MongoDB.

    Although the importance of MapReduce is now
    declining [5], it is still worth understanding, because it provides a clear picture of
    why and how batch processing is useful.

    In this chapter, we will look at MapReduce and several other batch processing algo‐
    rithms and frameworks, and explore how they are used in modern data systems. But
    first, to get started, we will look at data processing using standard Unix tools

    Batch Processing with Unix Tools
        Simple Log Analysis
            Various tools can take these log files and produce pretty reports about your website
            traffic, but for the sake of exercise, let’s build our own, using basic Unix tools. For
            example, say you want to find the five most popular pages on your website

                cat /var/log/nginx/access.log |
                 awk '{print $7}' |
                 sort |
                 uniq -c |
                 sort -r -n |
                 head -n 5

            Although the preceding command line likely looks a bit obscure if you’re unfamiliar
            with Unix tools, it is incredibly powerful. It will process gigabytes of log files in a
            matter of seconds, and you can easily modify the analysis to suit your needs

            Surprisingly many data analyses can be done in a few
            minutes using some combination of awk, sed, grep, sort, uniq, and xargs, and they
            perform surprisingly well

            Instead of the chain of Unix commands, you could write a simple program to do the
            same thing

            Which approach is better? It depends how many different URLs you have. For most
            small to mid-sized websites, you can probably fit all distinct URLs, and a counter for
            each URL, in (say) 1 GB of memory. In this example, the working set of the job (the
            amount of memory to which the job needs random access) depends only on the
            number of distinct URLs: if there are a million log entries for a single URL, the space
            required in the hash table is still just one URL plus the size of the counter. If this
            working set is small enough, an in-memory hash table works fine—even on a laptop.
            On the other hand, if the job’s working set is larger than the available memory, the
            sorting approach has the advantage that it can make efficient use of disks.
            chunks of
            data can be sorted in memory and written out to disk as segment files, and then mul‐
            tiple sorted segments can be merged into a larger sorted file.
            The sort utility in GNU Coreutils (Linux) automatically handles larger-than memory datasets
            by spilling to disk, and automatically parallelizes sorting across multiple CPU cores

        The Unix Philosophy
            the idea of connecting programs with pipes became part of what is now known as the Unix philosophy—

            The philosophy was described in 1978 as follows:
                1. Make each program do one thing well. To do a new job, build afresh rather than
                complicate old programs by adding new “features”.
                2. Expect the output of every program to become the input to another, as yet
                unknown, program. Don’t clutter output with extraneous information. Avoid
                stringently columnar or binary input formats. Don’t insist on interactive input.
                3. Design and build software, even operating systems, to be tried early, ideally within
                weeks. Don’t hesitate to throw away the clumsy parts and rebuild them.
                4. Use tools in preference to unskilled help to lighten a programming task, even if
                you have to detour to build the tools and expect to throw some of them out after
                you’ve finished using them.

            This approach—automation, rapid prototyping, incremental iteration, being friendly
            to experimentation, and breaking down large projects into manageable chunks—
            sounds remarkably like the Agile and DevOps movements of today. Surprisingly little
            has changed in four decades.

            A Unix shell like bash lets us easily compose these small programs into surprisingly
            powerful data processing jobs. Even though many of these programs are written by
            different groups of people, they can be joined together in flexible ways. What does
            Unix do to enable this composability?

            If you expect the output of one program to become the input to another program,
            that means those programs must use the same data format—in other words, a com‐
            patible interface. If you want to be able to connect any program’s output to any pro‐
            gram’s input, that means that all programs must use the same input/output interface.

            In Unix, that interface is a file (or, more precisely, a file descriptor). A file is just an
            ordered sequence of bytes. Because that is such a simple interface, many different
            things can be represented using the same interface: an actual file on the filesystem, a
            communication channel to another process (Unix socket, stdin, stdout), a device
            driver (say /dev/audio or /dev/lp0), a socket representing a TCP connection, and so
            on. It’s easy to take this for granted, but it’s actually quite remarkable that these very
            different things can share a uniform interface, so they can easily be plugged together.

            You can even write your own programs and combine them with the tools provided
            by the operating system. Your program just needs to read input from stdin and write
            output to stdout, and it can participate in data processing pipelines.

    MapReduce and Distributed Filesystems
!!!!    MapReduce is a bit like Unix tools, but distributed across potentially thousands of
        machines. Like Unix tools, it is a fairly blunt, brute-force, but surprisingly effective
        tool. A single MapReduce job is comparable to a single Unix process: it takes one or
        more inputs and produces one or more outputs

        As with most Unix tools, running a MapReduce job normally does not modify the
        input and does not have any side effects other than producing the output

!!!     While Unix tools use stdin and stdout as input and output, MapReduce jobs read
        and write files on a distributed filesystem. In Hadoop’s implementation of Map‐
        Reduce, that filesystem is called HDFS (Hadoop Distributed File System), an open
        source reimplementation of the Google File System (GFS)

        Various other distributed filesystems besides HDFS exist, such as GlusterFS and the
        Quantcast File System (QFS). Object storage services such as Amazon S3, Azure
        Blob Storage, and OpenStack Swift are similar in many ways.iv In this chapter we
        will mostly use HDFS as a running example, but the principles apply to any dis‐
        tributed filesystem.

!!!!    HDFS consists of a daemon process running on each machine, exposing a network
        service that allows other nodes to access files stored on that machine (assuming that
        every general-purpose machine in a datacenter has some disks attached to it). A cen‐
        tral server called the NameNode keeps track of which file blocks are stored on which
        machine. Thus, HDFS conceptually creates one big filesystem that can use the space
        on the disks of all machines running the daemon.

        In order to tolerate machine and disk failures, file blocks are replicated on multiple
        machines. Replication may mean simply several copies of the same data on multiple
        machines,

!!!!    HDFS has scaled well: at the time of writing, the biggest HDFS deployments run on
        tens of thousands of machines, with combined storage capacity of hundreds of peta‐
        bytes [23]. Such large scale has become viable because the cost of data storage and
        access on HDFS, using commodity hardware and open source software, is much
        lower than that of the equivalent capacity on a dedicated storage appliance

        MapReduce Job Execution
!!!         MapReduce is a programming framework with which you can write code to process
            large datasets in a distributed filesystem like HDFS. The easiest way of understanding
            it is by referring back to the web server log analysis example in “Simple Log Analysis”
            on page 391. The pattern of data processing in MapReduce is very similar to this
            example:
                1. Read a set of input files, and break it up into records. In the web server log exam‐
                ple, each record is one line in the log (that is, \n is the record separator).
                2. Call the mapper function to extract a key and value from each input record. In
                the preceding example, the mapper function is awk '{print $7}': it extracts the
                URL ($7) as the key, and leaves the value empty.
                3. Sort all of the key-value pairs by key. In the log example, this is done by the first
                sort command.
                4. Call the reducer function to iterate over the sorted key-value pairs. If there are
                multiple occurrences of the same key, the sorting has made them adjacent in the
                list, so it is easy to combine those values without having to keep a lot of state in
                memory. In the preceding example, the reducer is implemented by the command
                uniq -c, which counts the number of adjacent records with the same key

!!!!!!      Those four steps can be performed by one MapReduce job. Steps 2 (map) and 4
            (reduce) are where you write your custom data processing code. Step 1 (breaking files
            into records) is handled by the input format parser. Step 3, the sort step, is implicit
            in MapReduce—you don’t have to write it, because the output from the mapper is
            always sorted before it is given to the reducer.

            To create a MapReduce job, you need to implement two callback functions, the map‐
            per and reducer, which behave as follows:
                Mapper
                    The mapper is called once for every input record, and its job is to extract the key
                    and value from the input record. For each input, it may generate any number of
                    key-value pairs (including none). It does not keep any state from one input
                    record to the next, so each record is handled independently.
                Reducer
                    The MapReduce framework takes the key-value pairs produced by the mappers,
                    collects all the values belonging to the same key, and calls the reducer with an
                    iterator over that collection of values. The reducer can produce output records
                    (such as the number of occurrences of the same URL)

!!!!        The main difference from pipelines of Unix commands is that MapReduce can paral‐
            lelize a computation across many machines, without you having to write code to
            explicitly handle the parallelism. The mapper and reducer only operate on one record
            at a time; they don’t need to know where their input is coming from or their output is
            going to, so the framework can handle the complexities of moving data between
            machines.

            It is possible to use standard Unix tools as mappers and reducers in a distributed
            computation [25], but more commonly they are implemented as functions in a con‐
            ventional programming language. In Hadoop MapReduce, the mapper and reducer
            are each a Java class that implements a particular interface. In MongoDB and
            CouchDB, mappers and reducers are JavaScript functions

            Dan: see  "10-1 MapReduce example.png"

!!!         Each input file is typically hundreds of megabytes in size. The MapReduce scheduler
            (not shown in the diagram) tries to run each mapper on one of the machines that
            stores a replica of the input file, provided that machine has enough spare RAM and
            CPU resources to run the map task [26]. This principle is known as putting the com‐
            putation near the data [27]: it saves copying the input file over the network, reducing
            network load and increasing locality

!!!!        In most cases, the application code that should run in the map task is not yet present
            on the machine that is assigned the task of running it, so the MapReduce framework
            first copies the code (e.g., JAR files in the case of a Java program) to the appropriate
            machines. It then starts the map task and begins reading the input file, passing one
            record at a time to the mapper callback. The output of the mapper consists of key value pairs

!!!         The reduce side of the computation is also partitioned. While the number of map
            tasks is determined by the number of input file blocks, the number of reduce tasks is
            configured by the job author (it can be different from the number of map tasks). To
            ensure that all key-value pairs with the same key end up at the same reducer, the
            framework uses a hash of the key to determine which reduce task should receive a
            particular key-value pair

            The key-value pairs must be sorted, but the dataset is likely too large to be sorted with
            a conventional sorting algorithm on a single machine. Instead, the sorting is per‐
            formed in stages. First, each map task partitions its output by reducer, based on the
            hash of the key. Each of these partitions is written to a sorted file on the mapper’s
            local disk
            Whenever a mapper finishes reading its input file and writing its sorted output files,
            the MapReduce scheduler notifies the reducers that they can start fetching the output
            files from that mapper. The reducers connect to each of the mappers and download
            the files of sorted key-value pairs for their partition

!!!         The reduce task takes the files from the mappers and merges them together, preserv‐
            ing the sort order. Thus, if different mappers produced records with the same key,
            they will be adjacent in the merged reducer input.

            The reducer is called with a key and an iterator that incrementally scans over all
            records with the same key (which may in some cases not all fit in memory). The
            reducer can use arbitrary logic to process these records, and can generate any number
            of output records. These output records are written to a file on the distributed filesys‐
            tem (usually, one copy on the local disk of the machine running the reducer, with
            replicas on other machines).

!!!!!       The range of problems you can solve with a single MapReduce job is limited.
            Thus, it is very common for MapReduce jobs to be chained together into workflows,
            such that the output of one job becomes the input to the next job. The Hadoop Map‐
            Reduce framework does not have any particular support for workflows, so this chain‐
            ing is done implicitly by directory name: the first job must be configured to write its
            output to a designated directory in HDFS, and the second job must be configured to
            read that same directory name as its input. From the MapReduce framework’s point
            of view, they are two independent jobs.

            Chained MapReduce jobs are therefore less like pipelines of Unix commands (which
            pass the output of one process as input to another process directly, using only a small
            in-memory buffer) and more like a sequence of commands where each command’s
            output is written to a temporary file, and the next command reads from the tempo‐
            rary file. This design has advantages and disadvantages

!!!!        A batch job’s output is only considered valid when the job has completed successfully
            (MapReduce discards the partial output of a failed job). Therefore, one job in a work‐
            flow can only start when the prior jobs—that is, the jobs that produce its input direc‐
            tories—have completed successfully

        Reduce-Side Joins and Grouping
            In many datasets it is common for one record to have an association with another
            record: a foreign key in a relational model, a document reference in a document
            model, or an edge in a graph model. A join is necessary whenever you have some
            code that needs to access records on both sides of that association (both the record
            that holds the reference and the record being referenced)

            MapReduce has no concept of indexes—at least not in the usual sense.

            When a MapReduce job is given a set of files as input, it reads the entire content of all
            of those files; a database would call this operation a full table scan. If you only want to
            read a small number of records, a full table scan is outrageously expensive compared
            to an index lookup. However, in analytic queries (see “Transaction Processing or
            Analytics?” on page 90) it is common to want to calculate aggregates over a large
            number of records. In this case, scanning the entire input might be quite a reasonable
            thing to do, especially if you can parallelize the processing across multiple machines.

            When we talk about joins in the context of batch processing, we mean resolving all
            occurrences of some association within a dataset. For example, we assume that a job
            is processing the data for all users simultaneously, not merely looking up the data for
            one particular user (which would be done far more efficiently with an index).

            Example: analysis of user activity events
                Dan: nice example did not add notes here

!!          One way of looking at this architecture is that mappers “send messages” to the reduc‐
            ers. When a mapper emits a key-value pair, the key acts like the destination address
            to which the value should be delivered. Even though the key is just an arbitrary string
            (not an actual network address like an IP address and port number), it behaves like
            an address: all key-value pairs with the same key will be delivered to the same desti‐
            nation (a call to the reducer).

            Since MapReduce
            handles all network communication, it also shields the application code from having
            to worry about partial failures, such as the crash of another node: MapReduce trans‐
            parently retries failed tasks without affecting the application logic.

!!!         The pattern of “bringing all records with the same key to the same place” breaks
            down if there is a very large amount of data related to a single key. For example, in a
            social network, most users might be connected to a few hundred people, but a small
            number of celebrities may have many millions of followers. Such disproportionately
            active database records are known as linchpin objects [38] or hot keys.


        The Output of Batch Workflows
            In the case of database queries, we distinguished transaction processing (OLTP) pur‐
            poses from analytic purposes (see “Transaction Processing or Analytics?” on page
            90). We saw that OLTP queries generally look up a small number of records by key,
            using indexes, in order to present them to a user (for example, on a web page). On
            the other hand, analytic queries often scan over a large number of records, perform‐
            ing groupings and aggregations, and the output often has the form of a report: a
            graph showing the change in a metric over time, or the top 10 items according to
            some ranking, or a breakdown of some quantity into subcategories. The consumer of
            such a report is often an analyst or a manager who needs to make business decisions.
            Where does batch processing fit in? It is not transaction processing, nor is it analyt‐
            ics. It is closer to analytics, in that a batch process typically scans over large portions
            of an input dataset. However, a workflow of MapReduce jobs is not the same as a
            SQL query used for analytic purposes (see “Comparing Hadoop to Distributed Data‐
            bases” on page 414). The output of a batch process is often not a report, but some
            other kind of structure.

            Google’s original use of MapReduce was to build indexes for its search engine, which
            was implemented as a workflow of 5 to 10 MapReduce jobs [1]. Although Google
            later moved away from using MapReduce for this purpose [43], it helps to under‐
            stand MapReduce if you look at it through the lens of building a search index. (Even
            today, Hadoop MapReduce remains a good way of building indexes

            If you need to perform a full-text search over a fixed set of documents, then a batch
            process is a very effective way of building the indexes: the mappers partition the set of
            documents as needed, each reducer builds the index for its partition, and the index
            files are written to the distributed filesystem. Building such document-partitioned
            indexes (see “Partitioning and Secondary Indexes” on page 206) parallelizes very well.

            If the indexed set of documents changes, one option is to periodically rerun the entire
            indexing workflow for the entire set of documents, and replace the previous index
            files wholesale with the new index files when it is done. This approach can be compu‐
            tationally expensive if only a small number of documents have changed, but it has the
            advantage that the indexing process is very easy to reason about: documents in,
            indexes out.

!!!!        Search indexes are just one example of the possible outputs of a batch processing
            workflow. Another common use for batch processing is to build machine learning
            systems such as classifiers (e.g., spam filters, anomaly detection, image recognition)
            and recommendation systems (e.g., people you may know, products you may be
            interested in, or related searches

            The output of those batch jobs is often some kind of database: for example, a data‐
            base that can be queried by user ID to obtain suggested friends for that user, or a
            database that can be queried by product ID to get a list of related products

!!!!        These databases need to be queried from the web application that handles user
            requests, which is usually separate from the Hadoop infrastructure. So how does the
            output from the batch process get back into a database where the web application can
            query it?
            The most obvious choice might be to use the client library for your favorite database
            directly within a mapper or reducer, and to write from the batch job directly to the
            database server, one record at a time. This will work (assuming your firewall rules
            allow direct access from your Hadoop environment to your production databases),
            but it is a bad idea for several reasons:
                • As discussed previously in the context of joins, making a network request for
                every single record is orders of magnitude slower than the normal throughput of
                a batch task. Even if the client library supports batching, performance is likely to
                be poor.
                • MapReduce jobs often run many tasks in parallel. If all the mappers or reducers
                concurrently write to the same output database, with a rate expected of a batch
                process, that database can easily be overwhelmed, and its performance for queries
                is likely to suffer. This can in turn cause operational problems in other parts
                of the system [35].
                • Normally, MapReduce provides a clean all-or-nothing guarantee for job output:
                if a job succeeds, the result is the output of running every task exactly once, even
                if some tasks failed and had to be retried along the way; if the entire job fails, no
                output is produced. However, writing to an external system from inside a job
                produces externally visible side effects that cannot be hidden in this way. Thus,
                you have to worry about the results from partially completed jobs being visible to
                other systems, and the complexities of Hadoop task attempts and speculative
                execution.

            A much better solution is to build a brand-new database inside the batch job and
            write it as files to the job’s output directory in the distributed filesystem, just like the
            search indexes in the last section. Those data files are then immutable once written,
            and can be loaded in bulk into servers that handle read-only queries

            The handling of output from MapReduce jobs follows the same UNIX philosophy. By treat‐
            ing inputs as immutable and avoiding side effects (such as writing to external data‐
            bases), batch jobs not only achieve good performance but also become much easier to
            maintain:
                • If you introduce a bug into the code and the output is wrong or corrupted, you
                can simply roll back to a previous version of the code and rerun the job, and the
                output will be correct again. Or, even simpler, you can keep the old output in a
                different directory and simply switch back to it. Databases with read-write trans‐
                actions do not have this property: if you deploy buggy code that writes bad data
                to the database, then rolling back the code will do nothing to fix the data in the
                database. (The idea of being able to recover from buggy code has been called
                human fault tolerance [50].)
                • As a consequence of this ease of rolling back, feature development can proceed
                more quickly than in an environment where mistakes could mean irreversible
                damage. This principle of minimizing irreversibility is beneficial for Agile soft‐
                ware development [51].
                • If a map or reduce task fails, the MapReduce framework automatically reschedules it and runs it
                again on the same input. If the failure is due to a bug in
                the code, it will keep crashing and eventually cause the job to fail after a few
                attempts; but if the failure is due to a transient issue, the fault is tolerated. This
                automatic retry is only safe because inputs are immutable and outputs from
                failed tasks are discarded by the MapReduce framework.

        Comparing Hadoop to distributed databases
            The MapReduce approach is more appropriate for larger jobs: jobs that process so
            much data and run for such a long time that they are likely to experience at least one
            task failure along the way. In that case, rerunning the entire job due to a single task
            failure would be wasteful.

    Beyond MapReduce
        Although MapReduce became very popular and received a lot of hype in the late
        2000s, it is just one among many possible programming models for distributed sys‐
        tems. Depending on the volume of data, the structure of the data, and the type of pro‐
        cessing being done with it, other tools may be more appropriate for expressing a
        computation.

        In response to the difficulty of using MapReduce directly, various higher-level pro‐
        gramming models (Pig, Hive, Cascading, Crunch) were created as abstractions on top
        of MapReduce.

========================================================================================================================
Chapter 11. Stream Processing

























========================================================================================================================
Chapter x.



TODO    java mapreduce repo / example / tutorial
TODO    Hadoop
TODO FlumeJava
        https://hemantkgupta.medium.com/insights-from-paper-google-flumejava-easy-efficient-data-parallel-pipelines-155d0fc6db6

TODO https://neo4j.com/ graph database
TODO LevelDB [6] and RocksDB [7]